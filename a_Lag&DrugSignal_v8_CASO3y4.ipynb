{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne                                                              # Librería de python para explorar, visualizar,\n",
    "mne.set_log_level('WARNING')                                            # y analizar datos neurofisiológicos humanos.\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb                                                    \n",
    "color = ['green', 'blue','red','cyan', 'magenta', 'yellow','k','w']     # Paleta de colores para diferenciar las ondas\n",
    "\n",
    "import a_funciones as a_fun                                             # Funciones Alex\n",
    "import os                                                               # Para importar varios archivos\n",
    "\n",
    "xlabel = 'Muestra'                                                      # Abscisas\n",
    "ylabel = 'Amplitud (uV)'                                                # Ordenadas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a_Lag&DrugSignal_v7_case3_gauss.ipynb.\n",
    "##### En ésta version, se reemplaza el ERPTemplate por una función Gaussiana. El objetivo es mostrar el cambio en el rendimiento frente a una señal *estándar* y poder comparar dicho rendimiento con los obtenidos previamente al *drogar* la señal. se contemplan las dos opciones de insertar una función Gaussiana. Queda en comentario una línea de código en donde se puede multiplicar el ERPTemplate por la función Gaussiana.  \n",
    "##### *El procesamiento de los EEG se ejecutan en RAM: no se guardan en .csv.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nnp.savetxt(f'./a_results/meta_P300S02.csv', meta_P300S02, delimiter=',',fmt='%s')\\nfor j, cont_P300S01 in enumerate(meta_P300S01):   \\n    df = a_fun.to_df(cont_P300S01)\\n    titulo = (f'P300S01 con un desfase de {lag_flash[j]}')\\n    a_fun.grafic_8ch_test(df, titulo, xlabel, ylabel)  \\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carga y ajuste de la plantilla de un ERP seleccionado previamente.\n",
    "mat = scipy.io.loadmat('./dataset/ERPTemplate.mat')\n",
    "routput = mat['routput']\n",
    "erptemplate1 = routput[0][7][0][1][0][0][0][7]                              # erptemplate2 = routput[0][7][0][1][0][0][0][0] \n",
    "erptemplate1 = np.delete( erptemplate1, range(0,256,43),0)                  # erptemplate2 = np.delete( erptemplate2, range(0,256,43),0)\n",
    "\n",
    "#-------------------------------------\n",
    "# Observación general del DS ITBA: Pacientes experimento PASIVO -> P300S01,02,03,06. | Pacientes experimento ACTIVO: P300S04, 05, 07 y 08.\n",
    "# NOTA IMPORTANTE: SI DROGÁS LA SEÑAL ES PORQUE ESTÁS USANDO UNA TRAZA EEG DE PACIENTES EN MODALIDAD PASIVA.\n",
    "# Es decir, que no se están enfocando en nada particular. Con éstos EEG + ERP se crea el dataset sintético.\n",
    "#-------------------------------------\n",
    "\n",
    "# Carga de la señal EEG\n",
    "punto_mat = scipy.io.loadmat('./dataset/itba/P300S01.mat') \n",
    "signal = punto_mat['data'][0][0][0] \n",
    "\n",
    "'''\n",
    "df_signal = a_fun.to_df(signal)\n",
    "titulo = 'signal original'\n",
    "#a_fun.grafic_8ch_test(df_signal, titulo, xlabel, ylabel)  \n",
    "'''\n",
    "\n",
    "#t_trials = punto_mat['data'][0][0][3]\n",
    "t_flash = punto_mat['data'][0][0][4] \n",
    "#t_stim = mat['data'][0][0][2]\n",
    "#t_type = mat['data'][0][0][1]\n",
    "\n",
    "# Parámetros de la función gaussiana\n",
    "mu = 0      # Media\n",
    "sigma = 30   # Desviación estándar\n",
    "amp = 700  # Amplitud\n",
    "x = np.linspace(-125, 125, 250)\n",
    "\n",
    "# Calcular los valores de la función gaussiana\n",
    "f_gauss = amp * (1/(sigma * np.sqrt(2 * np.pi))) * np.exp(-((x - mu)**2) / (2 * sigma**2))\n",
    "# Adaptación de la función gaussiana a la misma estructura del ERPTemplate\n",
    "f_gaussx8 = np.empty((250,8), dtype=np.float64)\n",
    "for i in range(0,249):\n",
    "        temp_array = np.array([f_gauss[i], f_gauss[i], f_gauss[i], f_gauss[i], f_gauss[i], f_gauss[i], f_gauss[i], f_gauss[i]])\n",
    "        #temp_array = ([f_gauss[i], f_gauss[i], f_gauss[i], f_gauss[i], f_gauss[i], f_gauss[i], f_gauss[i], f_gauss[i]])\n",
    "        f_gaussx8[i]=temp_array\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------\n",
    "#-------------Vector principal donde se definen los cambios que tendrá la señal principal EEG.-------------\n",
    "#----------------------------------------------------------------------------------------------------------\n",
    "# CASO1: Reemplazo del ERPTemplate por una función gaussiana de mismo periodo (250 muestras) y similar amplitud.\n",
    "# CASO1B: Modificación en amplitud de la función gaussiana. Actúa como coeficiente. Lista con valores del 1 al 30 en intervalos de 0.3.\n",
    "# lag_flash = [x/3 for x in range(1, 51)] \n",
    "# CASO2: Modificación en fase del ERP. Desplaza la señal en unidades de muestras.\n",
    "lag_flash = [x for x in range(1, 21)] \n",
    "#----------------------------------------------------------------------------------------------------------\n",
    "#----------------------------------------------------------------------------------------------------------\n",
    "#----------------------------------------------------------------------------------------------------------\n",
    "# Genero un conjunto de archivos, cada uno de ellos es una variación de la señal ppal EEG desfasada.\n",
    "meta_P300S01 = np.empty(len(lag_flash), dtype=object)\n",
    "for j,lag in enumerate(lag_flash):   \n",
    "    punto_mat = scipy.io.loadmat('./dataset/itba/P300S01.mat') # print(f'Posición del vector lag_flash:{j} | Desfase de {lag} muestras')\n",
    "    signal = punto_mat['data'][0][0][0] \n",
    "    for i in range(0,4200):\n",
    "        if (t_flash[i,3]==2):\n",
    "            # signal[t_flash[i,0]-1:t_flash[i,0]+250-1,:] += (erptemplate1*3) # Línea original.\n",
    "            signal[t_flash[i,0]-1:t_flash[i,0]+250-1,:] += (f_gaussx8) # -> CASO 1.\n",
    "            # signal[t_flash[i,0]-1:t_flash[i,0]+250-1,:] += (f_gaussx8*lag) # -> CASO 1B.\n",
    "            # signal[t_flash[i,0]-lag:t_flash[i,0]+250-lag,:] += (f_gaussx8) # -> CASO 2.            \n",
    "    meta_P300S01[j] = signal\n",
    "\n",
    "# Opcional: Grabado en disco y graficado de todos los EEG procesados.\n",
    "'''\n",
    "np.savetxt(f'./a_results/meta_P300S02.csv', meta_P300S02, delimiter=',',fmt='%s')\n",
    "for j, cont_P300S01 in enumerate(meta_P300S01):   \n",
    "    df = a_fun.to_df(cont_P300S01)\n",
    "    titulo = (f'P300S01 con un desfase de {lag_flash[j]}')\n",
    "    a_fun.grafic_8ch_test(df, titulo, xlabel, ylabel)  \n",
    "'''     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4040 events found\n",
      "Event IDs: [ 1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "Not setting metadata\n",
      "4040 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 4040 events and 201 original time points ...\n",
      "0 bad epochs dropped\n",
      "\n",
      "-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-\n",
      "4004 events found\n",
      "Event IDs: [1 2]\n",
      "Not setting metadata\n",
      "3333 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 3333 events and 201 original time points ...\n",
      "0 bad epochs dropped\n",
      "\n",
      "-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-\n",
      "4004 events found\n",
      "Event IDs: [1 2]\n",
      "Not setting metadata\n",
      "671 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 671 events and 201 original time points ...\n",
      "0 bad epochs dropped\n",
      "\n",
      "-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-\n",
      "4004 events found\n",
      "Event IDs: [1 2]\n",
      "Not setting metadata\n",
      "4004 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 4004 events and 201 original time points ...\n",
      "0 bad epochs dropped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexc\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_prom_hnh:  0.7664902998236331\n",
      "4040 events found\n",
      "Event IDs: [ 1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "Not setting metadata\n",
      "4040 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 4040 events and 201 original time points ...\n",
      "0 bad epochs dropped\n",
      "\n",
      "-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-\n",
      "4004 events found\n",
      "Event IDs: [1 2]\n",
      "Not setting metadata\n",
      "3333 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 3333 events and 201 original time points ...\n",
      "0 bad epochs dropped\n",
      "\n",
      "-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-\n",
      "4004 events found\n",
      "Event IDs: [1 2]\n",
      "Not setting metadata\n",
      "671 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 671 events and 201 original time points ...\n",
      "0 bad epochs dropped\n",
      "\n",
      "-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-\n",
      "4004 events found\n",
      "Event IDs: [1 2]\n",
      "Not setting metadata\n",
      "4004 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 4004 events and 201 original time points ...\n",
      "0 bad epochs dropped\n",
      "accuracy_prom_hnh:  0.7664902998236331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexc\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4040 events found\n",
      "Event IDs: [ 1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "Not setting metadata\n",
      "4040 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 4040 events and 201 original time points ...\n",
      "0 bad epochs dropped\n",
      "\n",
      "-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-\n",
      "4004 events found\n",
      "Event IDs: [1 2]\n",
      "Not setting metadata\n",
      "3333 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 3333 events and 201 original time points ...\n",
      "0 bad epochs dropped\n",
      "\n",
      "-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-\n",
      "4004 events found\n",
      "Event IDs: [1 2]\n",
      "Not setting metadata\n",
      "671 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 671 events and 201 original time points ...\n",
      "0 bad epochs dropped\n",
      "\n",
      "-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-\n",
      "4004 events found\n",
      "Event IDs: [1 2]\n",
      "Not setting metadata\n",
      "4004 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 4004 events and 201 original time points ...\n",
      "0 bad epochs dropped\n",
      "accuracy_prom_hnh:  0.7664902998236331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexc\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4040 events found\n",
      "Event IDs: [ 1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "Not setting metadata\n",
      "4040 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 4040 events and 201 original time points ...\n",
      "0 bad epochs dropped\n",
      "\n",
      "-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-\n",
      "4004 events found\n",
      "Event IDs: [1 2]\n",
      "Not setting metadata\n",
      "3333 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 3333 events and 201 original time points ...\n",
      "0 bad epochs dropped\n",
      "\n",
      "-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-\n",
      "4004 events found\n",
      "Event IDs: [1 2]\n",
      "Not setting metadata\n",
      "671 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 671 events and 201 original time points ...\n",
      "0 bad epochs dropped\n",
      "\n",
      "-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-\n",
      "4004 events found\n",
      "Event IDs: [1 2]\n",
      "Not setting metadata\n",
      "4004 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 4004 events and 201 original time points ...\n",
      "0 bad epochs dropped\n",
      "accuracy_prom_hnh:  0.7664902998236331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexc\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4040 events found\n",
      "Event IDs: [ 1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "Not setting metadata\n",
      "4040 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 4040 events and 201 original time points ...\n",
      "0 bad epochs dropped\n",
      "\n",
      "-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-\n",
      "4004 events found\n",
      "Event IDs: [1 2]\n",
      "Not setting metadata\n",
      "3333 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 3333 events and 201 original time points ...\n",
      "0 bad epochs dropped\n",
      "\n",
      "-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-\n",
      "4004 events found\n",
      "Event IDs: [1 2]\n",
      "Not setting metadata\n",
      "671 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 671 events and 201 original time points ...\n",
      "0 bad epochs dropped\n",
      "\n",
      "-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-\n",
      "4004 events found\n",
      "Event IDs: [1 2]\n",
      "Not setting metadata\n",
      "4004 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 4004 events and 201 original time points ...\n",
      "0 bad epochs dropped\n",
      "accuracy_prom_hnh:  0.7664902998236331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexc\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4040 events found\n",
      "Event IDs: [ 1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "Not setting metadata\n",
      "4040 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 4040 events and 201 original time points ...\n",
      "0 bad epochs dropped\n",
      "\n",
      "-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-\n",
      "4004 events found\n",
      "Event IDs: [1 2]\n",
      "Not setting metadata\n",
      "3333 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 3333 events and 201 original time points ...\n",
      "0 bad epochs dropped\n",
      "\n",
      "-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-\n",
      "4004 events found\n",
      "Event IDs: [1 2]\n",
      "Not setting metadata\n",
      "671 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 671 events and 201 original time points ...\n",
      "0 bad epochs dropped\n",
      "\n",
      "-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-\n",
      "4004 events found\n",
      "Event IDs: [1 2]\n",
      "Not setting metadata\n",
      "4004 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 4004 events and 201 original time points ...\n",
      "0 bad epochs dropped\n",
      "accuracy_prom_hnh:  0.7664902998236331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexc\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4040 events found\n",
      "Event IDs: [ 1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "Not setting metadata\n",
      "4040 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 4040 events and 201 original time points ...\n",
      "0 bad epochs dropped\n",
      "\n",
      "-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-\n",
      "4004 events found\n",
      "Event IDs: [1 2]\n",
      "Not setting metadata\n",
      "3333 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 3333 events and 201 original time points ...\n",
      "0 bad epochs dropped\n",
      "\n",
      "-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-\n",
      "4004 events found\n",
      "Event IDs: [1 2]\n",
      "Not setting metadata\n",
      "671 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 671 events and 201 original time points ...\n",
      "0 bad epochs dropped\n",
      "\n",
      "-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-\n",
      "4004 events found\n",
      "Event IDs: [1 2]\n",
      "Not setting metadata\n",
      "4004 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 4004 events and 201 original time points ...\n",
      "0 bad epochs dropped\n",
      "accuracy_prom_hnh:  0.7664902998236331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexc\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4040 events found\n",
      "Event IDs: [ 1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "Not setting metadata\n",
      "4040 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 4040 events and 201 original time points ...\n",
      "0 bad epochs dropped\n",
      "\n",
      "-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-\n",
      "4004 events found\n",
      "Event IDs: [1 2]\n",
      "Not setting metadata\n",
      "3333 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 3333 events and 201 original time points ...\n",
      "0 bad epochs dropped\n",
      "\n",
      "-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-\n",
      "4004 events found\n",
      "Event IDs: [1 2]\n",
      "Not setting metadata\n",
      "671 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 671 events and 201 original time points ...\n",
      "0 bad epochs dropped\n",
      "\n",
      "-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-\n",
      "4004 events found\n",
      "Event IDs: [1 2]\n",
      "Not setting metadata\n",
      "4004 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 4004 events and 201 original time points ...\n",
      "0 bad epochs dropped\n",
      "accuracy_prom_hnh:  0.7664902998236331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexc\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4040 events found\n",
      "Event IDs: [ 1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "Not setting metadata\n",
      "4040 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 4040 events and 201 original time points ...\n",
      "0 bad epochs dropped\n",
      "\n",
      "-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-\n",
      "4004 events found\n",
      "Event IDs: [1 2]\n",
      "Not setting metadata\n",
      "3333 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 3333 events and 201 original time points ...\n",
      "0 bad epochs dropped\n",
      "\n",
      "-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-\n",
      "4004 events found\n",
      "Event IDs: [1 2]\n",
      "Not setting metadata\n",
      "671 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 671 events and 201 original time points ...\n",
      "0 bad epochs dropped\n",
      "\n",
      "-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-\n",
      "4004 events found\n",
      "Event IDs: [1 2]\n",
      "Not setting metadata\n",
      "4004 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 4004 events and 201 original time points ...\n",
      "0 bad epochs dropped\n",
      "accuracy_prom_hnh:  0.7664902998236331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexc\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4040 events found\n",
      "Event IDs: [ 1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "Not setting metadata\n",
      "4040 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 4040 events and 201 original time points ...\n",
      "0 bad epochs dropped\n",
      "\n",
      "-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-\n",
      "4004 events found\n",
      "Event IDs: [1 2]\n",
      "Not setting metadata\n",
      "3333 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 3333 events and 201 original time points ...\n",
      "0 bad epochs dropped\n",
      "\n",
      "-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-\n",
      "4004 events found\n",
      "Event IDs: [1 2]\n",
      "Not setting metadata\n",
      "671 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 671 events and 201 original time points ...\n",
      "0 bad epochs dropped\n",
      "\n",
      "-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-\n",
      "4004 events found\n",
      "Event IDs: [1 2]\n",
      "Not setting metadata\n",
      "4004 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 4004 events and 201 original time points ...\n",
      "0 bad epochs dropped\n",
      "accuracy_prom_hnh:  0.7664902998236331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexc\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4040 events found\n",
      "Event IDs: [ 1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "Not setting metadata\n",
      "4040 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 4040 events and 201 original time points ...\n",
      "0 bad epochs dropped\n",
      "\n",
      "-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-\n",
      "4004 events found\n",
      "Event IDs: [1 2]\n",
      "Not setting metadata\n",
      "3333 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 3333 events and 201 original time points ...\n",
      "0 bad epochs dropped\n",
      "\n",
      "-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-\n",
      "4004 events found\n",
      "Event IDs: [1 2]\n",
      "Not setting metadata\n",
      "671 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 671 events and 201 original time points ...\n",
      "0 bad epochs dropped\n",
      "\n",
      "-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-\n",
      "4004 events found\n",
      "Event IDs: [1 2]\n",
      "Not setting metadata\n",
      "4004 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 4004 events and 201 original time points ...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11112/676660177.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[0mepocked\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m     \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetlabels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meeg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meeg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m \u001b[1;34m'first'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'second'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m          \u001b[1;31m# OJO CON ESTA LÍNEA: JUNTA LOS HITs/NO HITs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[1;31m# Downsample the original FS=250 Hz signal to >>> 20 Hz | epochs.resample(20, npad=\"auto\") | stimepochs.resample(20, npad=\"auto\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11112/676660177.py\u001b[0m in \u001b[0;36mgetlabels\u001b[1;34m(eeg_mne, eeg_events, event_id)\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[0mreject\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[0mevent_times\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmne\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_events\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meeg_events\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstim_channel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m't_type'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshortest_event\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_duration\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.000001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconsecutive\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m         epochs = mne.Epochs(eeg_mne, event_times, event_id, tmin, tmax, proj=False,\n\u001b[0m\u001b[0;32m     76\u001b[0m                         \u001b[0mbaseline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreject\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreject\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreload\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m                         verbose=True, reject_by_annotation=None)\n",
      "\u001b[1;32m<decorator-gen-259>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, raw, events, event_id, tmin, tmax, baseline, picks, preload, reject, flat, proj, decim, reject_tmin, reject_tmax, detrend, on_missing, reject_by_annotation, metadata, event_repeated, verbose)\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\alexc\\anaconda3\\lib\\site-packages\\mne\\epochs.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, raw, events, event_id, tmin, tmax, baseline, picks, preload, reject, flat, proj, decim, reject_tmin, reject_tmax, detrend, on_missing, reject_by_annotation, metadata, event_repeated, verbose)\u001b[0m\n\u001b[0;32m   2568\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2569\u001b[0m         \u001b[1;31m# call BaseEpochs constructor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2570\u001b[1;33m         super(Epochs, self).__init__(\n\u001b[0m\u001b[0;32m   2571\u001b[0m             \u001b[0minfo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevent_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtmin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtmax\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2572\u001b[0m             \u001b[0mmetadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbaseline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbaseline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpicks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpicks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<decorator-gen-247>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, info, data, events, event_id, tmin, tmax, baseline, raw, picks, reject, flat, decim, reject_tmin, reject_tmax, detrend, proj, on_missing, preload_at_end, selection, drop_log, filename, metadata, event_repeated, verbose, raw_sfreq, annotations)\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\alexc\\anaconda3\\lib\\site-packages\\mne\\epochs.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    584\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreload\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# this will do the projection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mproj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mTrue\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_projector\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m             \u001b[1;31m# let's make sure we project if data was provided and proj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\alexc\\anaconda3\\lib\\site-packages\\mne\\epochs.py\u001b[0m in \u001b[0;36mload_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    641\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreload\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    642\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 643\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    644\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreload\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    645\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_baseline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<decorator-gen-253>\u001b[0m in \u001b[0;36m_get_data\u001b[1;34m(self, out, picks, item, units, tmin, tmax, verbose)\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\alexc\\anaconda3\\lib\\site-packages\\mne\\epochs.py\u001b[0m in \u001b[0;36m_get_data\u001b[1;34m(self, out, picks, item, units, tmin, tmax, verbose)\u001b[0m\n\u001b[0;32m   1444\u001b[0m                         \u001b[0mepoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1445\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# from disk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1446\u001b[1;33m                     \u001b[0mepoch_noproj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_epoch_from_raw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1447\u001b[0m                     epoch_noproj = self._detrend_offset_decim(\n\u001b[0;32m   1448\u001b[0m                         epoch_noproj, detrend_picks)\n",
      "\u001b[1;32m<decorator-gen-260>\u001b[0m in \u001b[0;36m_get_epoch_from_raw\u001b[1;34m(self, idx, verbose)\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\alexc\\anaconda3\\lib\\site-packages\\mne\\epochs.py\u001b[0m in \u001b[0;36m_get_epoch_from_raw\u001b[1;34m(self, idx, verbose)\u001b[0m\n\u001b[0;32m   2616\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2617\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'    Getting epoch for %d-%d'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2618\u001b[1;33m         data = self._raw._check_bad_segment(start, stop, self.picks,\n\u001b[0m\u001b[0;32m   2619\u001b[0m                                             \u001b[0mreject_start\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreject_stop\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2620\u001b[0m                                             self.reject_by_annotation)\n",
      "\u001b[1;32mc:\\Users\\alexc\\anaconda3\\lib\\site-packages\\mne\\io\\base.py\u001b[0m in \u001b[0;36m_check_bad_segment\u001b[1;34m(self, start, stop, picks, reject_start, reject_stop, reject_by_annotation)\u001b[0m\n\u001b[0;32m    470\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mdescr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'bad'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    471\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mdescr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 472\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpicks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_times\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    473\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    474\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\alexc\\anaconda3\\lib\\site-packages\\mne\\io\\base.py\u001b[0m in \u001b[0;36m_getitem\u001b[1;34m(self, item, return_times)\u001b[0m\n\u001b[0;32m    750\u001b[0m         \u001b[0msel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parse_get_set_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    751\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreload\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 752\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    753\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    754\u001b[0m             data = self._read_segment(start=start, stop=stop, sel=sel,\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#----------------------------------------------------------------------------------------------------------\n",
    "#----------Ciclo principal: Carga, preprocesamiento, obtención hits/no hits, testeo y resultados.----------\n",
    "#----------------------------------------------------------------------------------------------------------\n",
    "for ii, path in enumerate(meta_P300S01):\n",
    "#------------------------------------------------------\n",
    "#----------Etapa 1: Carga y preprocesamiento.----------\n",
    "#------------------------------------------------------\n",
    "    results_mne=f'./a_results/Resultados Paciente P300S01_0{ii}.csv' # Se puede cambiar a .txt pero NO disminuye el tamaño del archivo.\n",
    "    file_temp=open(results_mne,\"w\") \n",
    "    print(f', P300S01_{ii}', file=file_temp) # COLUMNA0.\n",
    "    punto_mat = scipy.io.loadmat('./dataset/itba/P300S01.mat')\n",
    "    \n",
    "    signal = path\n",
    "    \n",
    "    t_trials = punto_mat['data'][0][0][3]\n",
    "    t_flash = punto_mat['data'][0][0][4]\n",
    "    t_stim = punto_mat['data'][0][0][2]\n",
    "    t_type = punto_mat['data'][0][0][1]\n",
    "    \n",
    "    ch_names=[ 'Fz'  ,  'Cz',    'P3' ,   'Pz'  ,  'P4'  ,  'PO7'   , 'PO8'   , 'Oz']\n",
    "    ch_types= ['eeg'] * signal.shape[1]\n",
    "    ch_names_events = ch_names + ['t_stim']+ ['t_type']\n",
    "    ch_types_events = ch_types + ['misc'] + ['misc']\n",
    "    \n",
    "    #info = mne.create_info(ch_names, 250, ch_types=ch_types)\n",
    "    #eeg_mne = mne.io.array.RawArray(signal.T, info)\n",
    "    signal_events = np.concatenate([signal, t_stim, t_type],1)\n",
    "    info_events = mne.create_info(ch_names_events,250, ch_types_events)\n",
    "    eeg = mne.io.RawArray(signal_events.T, info_events)\n",
    "    \n",
    "    #fig=eeg.plot_psd()\n",
    "    eeg.filter(1,20)\n",
    "    #fig=eeg.plot_psd()\n",
    "    event_times = mne.find_events(eeg, stim_channel='t_type') \n",
    "    #eeg.plot(scalings='auto',n_channels=8,events=event_times,block=True)   # scalings=10e-05\n",
    "    \n",
    "    if (np.unique(t_flash[:,0]).shape[0] != 4200):                          # evalúa si el \"sample point id\" de t_flash tiene el tamaño correcto.\n",
    "        u,c = np.unique( t_flash[:,0], return_counts=True)                  # u->Los elementos únicos. c->la cantidad de veces que se repiten.\n",
    "        dup = u[c>1]                                                        # dup->Los que estén repetidos mas de una vez.    \n",
    "        \n",
    "        for i in range(dup.shape[0]):\n",
    "            idx = np.where( t_flash[:,0] == dup[i] )[0][0]\n",
    "            t_flash[idx,0]  -= 1\n",
    "            t_flash[idx,1]  = 1\n",
    "            t_type[t_flash[idx,0]] = t_flash[idx,3]\n",
    "            t_stim[t_flash[idx,0]] = t_flash[idx,2]\n",
    "\n",
    "    np.unique(t_flash[:,0]).shape\n",
    "    assert  np.unique(t_flash[:,0]).shape[0] == 4200, 'Problem with experiment structure.  There aren''t enough events.'\n",
    "#--------------------------------------------------\n",
    "#----------Etapa 2: Obtener hits/no hits.----------\n",
    "#--------------------------------------------------\n",
    "    def getstims(eeg_mne, eeg_events):                                      # Get the stimulations.  These are the FLASHINGS of rows and columns.\n",
    "        tmin = 0 \n",
    "        tmax = 0.8\n",
    "        reject = None\n",
    "        event_times = mne.find_events(eeg_events, stim_channel='t_stim',shortest_event=0, verbose=True, min_duration=0.000001, consecutive=True)\n",
    "                        \n",
    "        event_id = {'Row1':1,'Row2':2,'Row3':3,'Row4':4,'Row5':5,'Row6':6,'Col1':7,'Col2':8,'Col3':9,'Col4':10,'Col5':11,'Col6':12}\n",
    "\n",
    "        epochs = mne.Epochs(eeg_mne, event_times, event_id, tmin, tmax, proj=False,\n",
    "                        baseline=None, reject=reject, preload=True,\n",
    "                        verbose=True, reject_by_annotation=None)\n",
    "        \n",
    "        stims = event_times[:,-1]\n",
    "        return [epochs,stims]\n",
    "    stimepochs, stims = getstims(eeg, eeg)\n",
    "    \n",
    "    def getlabels(eeg_mne, eeg_events, event_id):\n",
    "        print(f'\\n-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-')\n",
    "        tmin = 0\n",
    "        tmax = 0.8\n",
    "        reject = None\n",
    "        event_times = mne.find_events(eeg_events, stim_channel='t_type', shortest_event=0, verbose=True, min_duration=0.000001, consecutive=True)\n",
    "        epochs = mne.Epochs(eeg_mne, event_times, event_id, tmin, tmax, proj=False,\n",
    "                        baseline=None, reject=reject, preload=True,\n",
    "                        verbose=True, reject_by_annotation=None)\n",
    "        labels = epochs.events[:, -1]\n",
    "        return [epochs, labels]\n",
    "    \n",
    "    # Tupla de tres valores: epochs.get_data().shape . Tomo uno por uno\n",
    "    # epocked: Objeto de tipo <class 'mne.evoked.EvokedArray'>. \n",
    "    # En epocked.data puedo sacar info pero por ahora no hay nada. \n",
    "    # El profesor lo usa para visualizar: evoked.plot(). De acá se pueden sacar segmentos promedio.\n",
    "    epochs, labels = getlabels(eeg, eeg, {'first':1}) # -> {'first':1}: Nohits.\n",
    "    epocked = epochs.average()\n",
    "    \n",
    "    epochs, labels = getlabels(eeg, eeg, {'second':2}) # -> {'second':2}: Hits.\n",
    "    epocked = epochs.average()\n",
    "        \n",
    "    epochs, labels = getlabels(eeg, eeg, { 'first':1, 'second':2})          # OJO CON ESTA LÍNEA: JUNTA LOS HITs/NO HITs\n",
    "    \n",
    "    # Downsample the original FS=250 Hz signal to >>> 20 Hz | epochs.resample(20, npad=\"auto\") | stimepochs.resample(20, npad=\"auto\")\n",
    "    repetitions=120\n",
    "#----------------------------------------------------------------------------------\n",
    "#----------Etapa 3: Testeo de los cambios: modelo de regresión logística.----------\n",
    "#----------------------------------------------------------------------------------\n",
    "    # This is Single Flashing Classification attempt.\n",
    "    from sklearn.preprocessing import  StandardScaler\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn import svm\n",
    "    from sklearn.pipeline import make_pipeline\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, f1_score # support\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "    from mne.decoding import LinearModel                                    # import a linear classifier from mne.decoding\n",
    "    import matplotlib.pyplot as plt\n",
    "    clf = LogisticRegression(solver='lbfgs')\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    model = LinearModel(clf)                                                # create a linear model with LogisticRegression\n",
    "\n",
    "    # Get the epoched data (get only the data columns)\n",
    "    eeg_data2 = epochs.get_data() # eeg_data2.shape -> (4004, 10, 201): 4004 events found, 201 time points and 10 channels\n",
    "    eeg_data = epochs.get_data().reshape(len(labels), -1) # Obtener epochs, reorganiza en arreglo con las etiquetas \"labels\".\n",
    "    eeg_data3 = eeg_data\n",
    "    eeg_data = eeg_data[:,0:epochs.get_data().shape[2]*1] # Selección de todas las filas y recorte de matriz: dos primeras columnas.\n",
    "    #eeg_data[labels==2] = erptemplate1[:201,0]\n",
    "    #eeg_data[labels==1] = erptemplate1[:201,0]\n",
    "\n",
    "    #eeg_data[labels==2] = np.zeros((eeg_data.shape[1],))\n",
    "    #eeg_data[labels==1] = np.ones((eeg_data.shape[1],))\n",
    "    #labels = np.random.permutation(labels)\n",
    "\n",
    "    X = scaler.fit_transform(eeg_data) # Ajuste del clasificador. Estandarizar los datos: media de cero y una desviación estándar de uno. X.shape = (4004, 201)\n",
    "\n",
    "    model.fit(X[0:2800], labels[0:2800]) # Entrenamiento con los primeros 2800 elementos de X y las etiquetas \"labels\".\n",
    "\n",
    "    preds = model.predict(X[2800:]) # Predicción con el resto de los 2800 elementos.\n",
    "#------------------------------------------------------\n",
    "#----------Etapa 4: Generación de resultados.----------\n",
    "#------------------------------------------------------\n",
    "    target_names = ['nohit', 'hit']\n",
    "    report = classification_report(labels[2800:], preds, target_names=target_names)\n",
    "    report_dict = classification_report(labels[2800:], preds, target_names=target_names, output_dict=True)\n",
    "    \n",
    "    print(f'Muestra, {int(ii+1)}', file=file_temp)\n",
    "    print(f'Coef_erp, {lag_flash[ii]}', file=file_temp)\n",
    "\n",
    "    accuracy_hits = report_dict['hit']['precision']\n",
    "    print(f'accuracy_hits, {accuracy_hits}', file=file_temp)\n",
    "    accuracy_nohits = report_dict['nohit']['precision']\n",
    "    print(f'accuracy_nohits, {accuracy_nohits}', file=file_temp)\n",
    "    \n",
    "    accuracy_prom_hnh = (accuracy_hits + accuracy_nohits)/2\n",
    "    print(f'accuracy_prom_hnh, {accuracy_prom_hnh}', file=file_temp)\n",
    "    print(\"accuracy_prom_hnh: \", accuracy_prom_hnh)\n",
    "\n",
    "    '''\n",
    "    nohit_recall = report_dict['nohit']['recall']\n",
    "    nohit_f1_score = report_dict['nohit']['f1-score']\n",
    "    nohit_support = report_dict['nohit']['support']\n",
    "    print(f'nohit_recall, {nohit_recall}', file=file_temp)\n",
    "    print(f'nohit_f1_score, {nohit_f1_score}', file=file_temp)\n",
    "    print(f'nohit_support, {nohit_support}', file=file_temp)\n",
    "\n",
    "    hit_recall = report_dict['hit']['recall']\n",
    "    hit_f1_score = report_dict['hit']['f1-score']\n",
    "    hit_support = report_dict['hit']['support']\n",
    "    print(f'hit_recall, {hit_recall}', file=file_temp)\n",
    "    print(f'hit_f1_score, {hit_f1_score}', file=file_temp)\n",
    "    print(f'hit_support, {hit_support}', file=file_temp)\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    cm = confusion_matrix(labels[2800:], preds)\n",
    "    print (cm)\n",
    "    cm_normalized = cm.astype(float) / cm.sum(axis=1)[:, np.newaxis]\n",
    "    acc=(cm[0,0]+cm[1,1])*1.0/(np.sum(cm))\n",
    "    '''    \n",
    "    \n",
    "    file_temp.close()\n",
    "#-----------------------------------------\n",
    "#---------- FIN Ciclo principal.----------\n",
    "#-----------------------------------------\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------\n",
    "#---------- Presentación de resultados en una tabla.----------\n",
    "#-------------------------------------------------------------\n",
    "path2 = './a_results/'\n",
    "columns_list = []\n",
    "\n",
    "for filename in os.listdir(path2):\n",
    "    if filename.startswith('Resultados Paciente') and filename.endswith('.csv'):\n",
    "        df = pd.read_csv(os.path.join(path2, filename), delimiter=',')\n",
    "        columns_list.append(df.iloc[:, 1])\n",
    "\n",
    "resultados = pd.concat(columns_list, axis=1)\n",
    "resultados.columns = [filename[:-4] for filename in os.listdir(path2) if filename.startswith('Resultados Paciente') and filename.endswith('.csv')]\n",
    "df1 = pd.read_csv('./a_results/Resultados Paciente P300S01_00.csv', index_col=0)\n",
    "resultados.set_index(df1.index, inplace=True)\n",
    "resultados.to_csv('./a_results/resultados.csv', index=False)\n",
    "resultados = resultados.T\n",
    "resultados = resultados.sort_values(by=['Muestra'],ascending=[True])\n",
    "resultados['Muestra'] = resultados['Muestra'].astype(int)\n",
    "#resultados['Indice'] = resultados['Muestra']-1\n",
    "#resultados['Experimento'] = resultados.index\n",
    "#resultados['Muestra'] = resultados['Muestra']-1\n",
    "#df_resultados = a_fun.to_df(resultados)\n",
    "resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------\n",
    "#---------- Presentación gráfica.----------\n",
    "#------------------------------------------\n",
    "plt.figure(figsize=(30,8))                                          \n",
    "axes = plt.gca()                                                    \n",
    "plt.plot(resultados['Coef_erp'], resultados['accuracy_hits'], '-o', label='hits') \n",
    "plt.plot(resultados['Coef_erp'], resultados['accuracy_nohits'], '-o',label='no hits') \n",
    "plt.plot(resultados['Coef_erp'], resultados['accuracy_prom_hnh'], '-o',label='average') \n",
    "#axes.set_title('Precision')\n",
    "axes.set_xlabel('Coef_erp (µV)')\n",
    "axes.set_ylabel('Precision')\n",
    "axes.title.set_size(30)\n",
    "axes.xaxis.label.set_size(20)\n",
    "axes.yaxis.label.set_size(20)                                  \n",
    "plt.grid()\n",
    "plt.legend(loc='center',fontsize='xx-large')\n",
    "plt.savefig(\"./a_images/resultados_caso4_a_precision.jpg\")\n",
    "plt.show()\n",
    "#------------------------------------------\n",
    "\n",
    "'''\n",
    "plt.figure(figsize=(30,8))                                          \n",
    "axes = plt.gca()                                                    \n",
    "plt.plot(resultados['Coef_erp'], resultados['hit_recall'], '-o', label='hits') \n",
    "plt.plot(resultados['Coef_erp'], resultados['nohit_recall'], '-o',label='no hits') \n",
    "#axes.set_title('Precision')\n",
    "axes.set_xlabel('Coef_erp (µV)')\n",
    "axes.set_ylabel('Recall')\n",
    "axes.title.set_size(30)\n",
    "axes.xaxis.label.set_size(20)\n",
    "axes.yaxis.label.set_size(20)                                  \n",
    "plt.grid()\n",
    "plt.legend(loc='center',fontsize='xx-large')\n",
    "plt.savefig(\"./a_images/resultados_caso4_b_recall.jpg\")\n",
    "plt.show()\n",
    "#------------------------------------------\n",
    "plt.figure(figsize=(30,8))                                          \n",
    "axes = plt.gca()                                                    \n",
    "plt.plot(resultados['Coef_erp'], resultados['hit_f1_score'], '-o', label='hits') \n",
    "plt.plot(resultados['Coef_erp'], resultados['nohit_f1_score'], '-o',label='no hits') \n",
    "#axes.set_title('Precision')\n",
    "axes.set_xlabel('Coef_erp (µV)')\n",
    "axes.set_ylabel('f1_score')\n",
    "axes.title.set_size(30)\n",
    "axes.xaxis.label.set_size(20)\n",
    "axes.yaxis.label.set_size(20)                                  \n",
    "plt.grid()\n",
    "plt.legend(loc='center',fontsize='xx-large')\n",
    "plt.savefig(\"./a_images/resultados_caso4_c_f1_score.jpg\")\n",
    "plt.show()\n",
    "#------------------------------------------\n",
    "plt.figure(figsize=(30,8))                                          \n",
    "axes = plt.gca()                                                    \n",
    "plt.plot(resultados['Coef_erp'], resultados['hit_support'], '-o', label='hits') \n",
    "plt.plot(resultados['Coef_erp'], resultados['nohit_support'], '-o',label='no hits') \n",
    "#axes.set_title('Precision')\n",
    "axes.set_xlabel('Coef_erp (µV)')\n",
    "axes.set_ylabel('Support')\n",
    "axes.title.set_size(30)\n",
    "axes.xaxis.label.set_size(20)\n",
    "axes.yaxis.label.set_size(20)                                  \n",
    "plt.grid()\n",
    "plt.legend(loc='center',fontsize='xx-large')\n",
    "plt.savefig(\"./a_images/resultados_caso4_d_support.jpg\")\n",
    "plt.show()\n",
    "#------------------------------------------\n",
    "\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =======================================\n",
    "# Por ahora hasta acá\n",
    "# ======================================="
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Acá puedo revisar esta guia -> [*Guía*](https://github.com/faturita/python-nerv/blob/master/MNE%20BNCI%20Horizon%202020%20Dataset%20008-2014.ipynb)    \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis de DrugSignal.py, parte II  \n",
    "  \n",
    "En *a_analisis_funcion_DrugSignal.ipynb* se llega a drogar la señal con variaciones en la amplitud y fase.  \n",
    "A continuación la preparación de los datos para ser visualizados con la librería de python mne.  \n",
    "  \n",
    "[*Creating data objects from arrays. mne library*](https://mne.tools/stable/creating_from_arrays.html)  \n",
    "  \n",
    "- Una instancia básica de información llamada *info_events*:  \n",
    "Esta contiene el nombre de los canales, el ratio de muestreo y el canal de datos.  \n",
    "Para éste caso no se usará canal de datos, por eso se carga *ch_types_events* con ['misc'].  \n",
    "  \n",
    "- Un objeto \"en crudo\" a partir de un array de numpy llamado *eeg*.  \n",
    "\n",
    "- Un tercer objeto *event_times* que trae la info completa y además agrega el 't_type',\n",
    "profundizado en [*a_analisis_p300subject25_stim&type.ipynb*](a_analisis_p300subject25_stim&type.ipynb).\n",
    "\n",
    "Al finalizar, tendremos un objeto *eeg* que contiene todo:  \n",
    "La señal drogada, *t_stim* y *t_type* y sus respectivos eventos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_names=['Fz','Cz','P3','Pz','P4','PO7','PO8', 'Oz']                   # Los nombres de los canales\n",
    "ch_types= ['eeg'] * signal.shape[1]                                     # Un vector de tamaño 8 con el'eeg' repetido\n",
    "ch_names_events = ch_names + ['t_stim']+ ['t_type']                     # Le agrega  't_stim' y't_type' \n",
    "ch_types_events = ch_types + ['misc'] + ['misc']                        # Le agrega dos 'misc': \n",
    "                                                                        # Channel types, default is 'misc' which is not a data channel. \n",
    "                                                                        # Currently supported fields are ‘ecg’, ‘bio’, ‘stim’, ‘eog’, ‘misc’, \n",
    "                                                                        # ‘seeg’, ‘dbs’, ‘ecog’, ‘mag’, ‘eeg’, ‘ref_meg’, ‘grad’, ‘emg’, ‘hbr’ or ‘hbo’. \n",
    "                                                                        # If str, then all channels are assumed to be of the same type.\n",
    "#info = mne.create_info(ch_names, 250, ch_types=ch_types)\n",
    "#eeg_mne = mne.io.array.RawArray(signal.T, info)\n",
    "\n",
    "signal_events = np.concatenate([signal, t_stim, t_type],1)              # Se agrega a la señal drogada 't_stim' y 't_type'\n",
    "\n",
    "df_signal_events = a_fun.to_df(signal_events)\n",
    "ch_names_events_with_sample = ch_names_events + ['sample']\n",
    "df_signal_events.columns = ch_names_events_with_sample\n",
    "\n",
    "info_events = mne.create_info(ch_names_events,250, ch_types_events)     # Acá crea los eventos. \n",
    "eeg = mne.io.RawArray(signal_events.T, info_events)                     # El objeto en crudo.\n",
    "\n",
    "# Do some basic signal processing (1-20 band pass filter)\n",
    "# *** fig=eeg.plot_psd()\n",
    "eeg.filter(1,20)\n",
    "# *** fig=eeg.plot_psd()\n",
    "event_times = mne.find_events(eeg, stim_channel='t_type')    \n",
    "# *** eeg.plot(scalings='auto',n_channels=8,events=event_times,block=True)   # scalings=10e-05\n",
    "\n",
    "#========================================================================\n",
    "# ChatGPT:\n",
    "# Primero, verifica si la cantidad de valores únicos en la primera columna de la matriz es igual a 4200. \n",
    "# Si no es así, entonces el código busca valores duplicados en la primera columna de la matriz \n",
    "# y disminuye el valor de la fila correspondiente en 1 y establece el valor de la columna 1 en 1. \n",
    "# Luego, establece los valores de las columnas 2 y 3 de la matriz t_type y t_stim según los valores\n",
    "# correspondientes de la columna 3 y 2 de la matriz t_flash.\n",
    "# En la última línea del código, se utiliza la función assert para asegurarse de que la cantidad de valores únicos en la primera columna de la matriz t_flash sea igual a 4200. Si no lo es, se imprimirá un mensaje de error indicando que hay un problema con la estructura del experimento.\n",
    "\n",
    "if (np.unique(t_flash[:,0]).shape[0] != 4200):                          # evalúa si el \"sample point id\" de t_flash tiene el tamaño correcto.\n",
    "    u,c = np.unique( t_flash[:,0], return_counts=True)                  # u->Los elementos únicos. c->la cantidad de veces que se repiten.\n",
    "    dup = u[c>1]                                                        # dup->Los que estén repetidos mas de una vez.    \n",
    "    \n",
    "    for i in range(dup.shape[0]):\n",
    "        idx = np.where( t_flash[:,0] == dup[i] )[0][0]\n",
    "        t_flash[idx,0]  -= 1\n",
    "        t_flash[idx,1]  = 1\n",
    "        t_type[t_flash[idx,0]] = t_flash[idx,3]\n",
    "        t_stim[t_flash[idx,0]] = t_flash[idx,2]\n",
    "\n",
    "np.unique(t_flash[:,0]).shape\n",
    "assert  np.unique(t_flash[:,0]).shape[0] == 4200, 'Problem with experiment structure.  There aren''t enough events.'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gráficas de la señal con el ERP agregado y con *t_stim y t_type*.   \n",
    "La primera gráfica muestra el resultado general.    \n",
    "La segunda gráfica es cada una de las señales en un periodo de tiempo determinado.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(30,8))                                     \n",
    "#axes = plt.gca()\n",
    "#for i in range(0,9):\n",
    "#    plt.plot(df_signal_events['sample'], df_signal_events.iloc[:,i])\n",
    "#axes.set_title('DrugSignal con t_stim y t_type'), axes.title.set_size(30)\n",
    "#axes.set_xlabel(xlabel), axes.set_ylabel(ylabel)\n",
    "#axes.xaxis.label.set_size(20), axes.yaxis.label.set_size(20)\n",
    "#plt.savefig(\"./a_images/DrugSignal_p300subject25[data][0][0][0].jpg\")\n",
    "#plt.grid(), plt.show()\n",
    "                 \n",
    "\n",
    "xlim_general=([7000,15500])\n",
    "ylim_general=([-50,80])\n",
    "eje_x_sample=df_signal_events['sample']\n",
    "fig, (ax0, ax1, ax2, ax3, ax4, ax5, ax6, ax7, ax8, ax9) = plt.subplots(nrows=10, ncols=1,figsize=(30,20))\n",
    "fig.suptitle(\"DrugSignal con t_stim y t_type  \\n Eje x: muestra. Eje y: uV \", fontsize=30)\n",
    "\n",
    "ax0.plot(eje_x_sample, df_signal_events.iloc[:,0], label='0') \n",
    "ax0.set_xlim(xlim_general), ax0.set_ylim(ylim_general), ax0.set_title(df_signal_events.columns[0]), ax0.grid()\n",
    "\n",
    "ax1.plot(eje_x_sample, df_signal_events.iloc[:,1], label='1')\n",
    "ax1.set_xlim(xlim_general), ax1.set_ylim(ylim_general), ax1.set_title(df_signal_events.columns[1]), ax1.grid()\n",
    "\n",
    "ax2.plot(eje_x_sample, df_signal_events.iloc[:,2], label='2')\n",
    "ax2.set_xlim(xlim_general), ax2.set_ylim(ylim_general), ax2.set_title(df_signal_events.columns[2]), ax2.grid()\n",
    "\n",
    "ax3.plot(eje_x_sample, df_signal_events.iloc[:,3], label='3')\n",
    "ax3.set_xlim(xlim_general), ax3.set_ylim(ylim_general), ax3.set_title(df_signal_events.columns[3]), ax3.grid()\n",
    "\n",
    "ax4.plot(eje_x_sample, df_signal_events.iloc[:,4], label='4')\n",
    "ax4.set_xlim(xlim_general), ax4.set_ylim(ylim_general), ax4.set_title(df_signal_events.columns[4]), ax4.grid()\n",
    "\n",
    "ax5.plot(eje_x_sample, df_signal_events.iloc[:,5], label='5')\n",
    "ax5.set_xlim(xlim_general), ax5.set_ylim(ylim_general), ax4.set_title(df_signal_events.columns[5]), ax5.grid()\n",
    "\n",
    "ax6.plot(eje_x_sample, df_signal_events.iloc[:,6], label='6')\n",
    "ax6.set_xlim(xlim_general), ax6.set_ylim(ylim_general), ax6.set_title(df_signal_events.columns[6]), ax6.grid()\n",
    "\n",
    "ax7.plot(eje_x_sample, df_signal_events.iloc[:,7], label='7')\n",
    "ax7.set_xlim(xlim_general), ax7.set_ylim(ylim_general), ax7.set_title(df_signal_events.columns[7]), ax7.grid()\n",
    "\n",
    "ax8.plot(eje_x_sample, df_signal_events.iloc[:,8], label='8')\n",
    "ax8.set_xlim(xlim_general), ax8.set_ylim([-3,14]), ax8.set_title(df_signal_events.columns[8]), ax8.grid()\n",
    "\n",
    "ax9.plot(eje_x_sample, df_signal_events.iloc[:,9], label='9')\n",
    "ax9.set_xlim(xlim_general), ax9.set_ylim([-0.2,2.2]), ax9.set_title(df_signal_events.columns[9]), ax9.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones para obtener las estimulaciones y etiquetas.\n",
    "  \n",
    "#### getstims.  \n",
    "La función usa *info*, que es creada con los nombres de los canales (ch_names), la frecuencia de muestreo (259) y los tipos de canales ch_types.  También toma como entrada *eeg_mne*; es la traspuesta de signal e info.\n",
    "\n",
    "El resultado *event_times* es llamando a la función *find_events* de mne, que encuentra eventos del archivo \"raw\".  \n",
    "\n",
    "[*\"Find events\"*](https://mne.tools/stable/generated/mne.find_events.html)  \n",
    "\n",
    "\n",
    "[*\"Épocas: INVESTIGAR. NO ENTENDI NADA\"*](https://mne.tools/stable/generated/mne.Epochs.html#mne-epochs)\n",
    "\n",
    "#### getlabels.\n",
    "\n",
    "- Una instancia básica de información llamada *info_events*:  \n",
    "Esta contiene el nombre de los canales, el ratio de muestreo y el canal de datos.  \n",
    "Para éste caso no se usará canal de datos, por eso se carga *ch_types_events* con ['misc'].  \n",
    "  \n",
    "- Un objeto \"en crudo\" a partir de un array de numpy llamado *eeg*.  \n",
    "\n",
    "- Un tercer objeto *event_times* que trae la info completa y además agrega el 't_type',\n",
    "profundizado en [*a_analisis_p300subject25_stim&type.ipynb*](a_analisis_p300subject25_stim&type.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#========================================================================\n",
    "# getstims recibe dos argumentos: eeg_mne y eeg_events. eeg_mne debe ser un objeto de tipo MNE Raw o Epochs, \n",
    "# mientras que eeg_events debe ser un objeto que contenga los eventos relacionados con la señal EEG.\n",
    "# La función utiliza la biblioteca MNE de Python para encontrar los eventos en la señal EEG y crear epochs de 0 a 0.8 segundos de duración, \n",
    "# a partir de los eventos encontrados. Luego, la función extrae las marcas de tiempo de los eventos y las devuelve como una lista junto \n",
    "# con los epochs.\n",
    "\n",
    "# En resumen, la función getstims extrae los estímulos de una señal EEG y devuelve una lista de epochs y las marcas de tiempo de los eventos \n",
    "# relacionados con la señal.\n",
    "\n",
    "def getstims(eeg_mne, eeg_events):\n",
    "    '''\n",
    "    Get the stimulations.  These are the FLASHINGS of rows and columns.\n",
    "    '''\n",
    "    tmin = 0\n",
    "    tmax = 0.8\n",
    "    reject = None\n",
    "    event_times = mne.find_events(eeg_events, stim_channel='t_stim',shortest_event=0, verbose=True, min_duration=0.000001, consecutive=True)\n",
    "    event_id = {'Row1':1,'Row2':2,'Row3':3,'Row4':4,'Row5':5,'Row6':6,'Col1':7,'Col2':8,'Col3':9,'Col4':10,'Col5':11,'Col6':12}\n",
    "\n",
    "\n",
    "    epochs = mne.Epochs(eeg_mne, event_times, event_id, tmin, tmax, proj=False,\n",
    "                    baseline=None, reject=reject, preload=True,\n",
    "                    verbose=True, reject_by_annotation=None)\n",
    "\n",
    "\n",
    "    stims = event_times[:,-1]\n",
    "\n",
    "    return [epochs,stims]\n",
    "\n",
    "stimepochs, stims = getstims(eeg, eeg)                                  # Época de estimulación.\n",
    "stimepochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#========================================================================\n",
    "# ChatGPT:\n",
    "# La función getlabels recibe tres argumentos: eeg_mne, eeg_events y event_id. eeg_mne debe ser un objeto de tipo MNE Raw o Epochs, \n",
    "# mientras que eeg_events debe ser un objeto que contenga los eventos relacionados con la señal EEG.\n",
    "# La función utiliza la biblioteca MNE de Python para encontrar los eventos en la señal EEG y crear epochs de 0 a 0.8 segundos de duración, \n",
    "# a partir de los eventos encontrados. Luego, la función extrae las marcas de tiempo de los eventos y las devuelve como una lista junto \n",
    "# con los epochs.\n",
    "\n",
    "# En resumen, la función getstims extrae los estímulos de una señal EEG y devuelve una lista de epochs y las marcas de tiempo de los eventos \n",
    "# relacionados con la señal.\n",
    "\n",
    "\n",
    "def getlabels(eeg_mne, eeg_events, event_id):\n",
    "    '''\n",
    "    Get the hit/no hits labels.  These are the FLASHINGS of rows and columns but selected if they are the ones that will\n",
    "    trigger the P300 response or not.\n",
    "    '''\n",
    "    #event_id = { 'first':1, 'second':2 }\n",
    "    #baseline = (0.0, 0.2)\n",
    "    #reject = {'eeg': 70 * pow(10,6)}\n",
    "    tmin = 0\n",
    "    tmax = 0.8\n",
    "    reject = None\n",
    "    event_times = mne.find_events(eeg_events, stim_channel='t_type', shortest_event=0, verbose=True, min_duration=0.000001, consecutive=True)\n",
    "    epochs = mne.Epochs(eeg_mne, event_times, event_id, tmin, tmax, proj=False,\n",
    "                    baseline=None, reject=reject, preload=True,\n",
    "                    verbose=True, reject_by_annotation=None)\n",
    "    labels = epochs.events[:, -1]\n",
    "    return [epochs, labels]\n",
    "\n",
    "epochs, labels = getlabels(eeg, eeg, {'first':1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "epocked = epochs.average()\n",
    "epocked.plot(window_title='NoHit Averaged Signals')\n",
    "\n",
    "\n",
    "epochs, labels = getlabels(eeg, eeg, {'second':2})\n",
    "\n",
    "epocked = epochs.average()\n",
    "epocked.plot(window_title='Hit Averaged Signals')\n",
    "\n",
    "epochs, labels = getlabels(eeg, eeg, { 'first':1, 'second':2})\n",
    "\n",
    "\n",
    "# Downsample the original FS=250 Hz signal to >>> 20 Hz\n",
    "#epochs.resample(20, npad=\"auto\")\n",
    "#stimepochs.resample(20, npad=\"auto\")\n",
    "repetitions=120"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6b958d2ee4113081da5b7324adcb012071bef3fd829261c203699a43a2ce6bc6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
