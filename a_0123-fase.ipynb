{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4382e7d8-779b-4368-820d-cffc49060cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector_desfase:  [1, 2, 3, 4, 5]\n",
      "4040 events found\n",
      "Event IDs: [ 1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "Not setting metadata\n",
      "4040 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 4040 events and 201 original time points ...\n",
      "0 bad epochs dropped\n",
      "4004 events found\n",
      "Event IDs: [1 2]\n",
      "Not setting metadata\n",
      "3333 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 3333 events and 201 original time points ...\n",
      "0 bad epochs dropped\n",
      "4004 events found\n",
      "Event IDs: [1 2]\n",
      "Not setting metadata\n",
      "671 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 671 events and 201 original time points ...\n",
      "0 bad epochs dropped\n",
      "4004 events found\n",
      "Event IDs: [1 2]\n",
      "Not setting metadata\n",
      "4004 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 4004 events and 201 original time points ...\n",
      "0 bad epochs dropped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexc\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hlbls: 33\n",
      "hlbls: 33\n",
      "9_944___9_____9_________4___9_4_2\n",
      "Performance Classification of Averaged Epochs\n",
      "_9_________4___9_4_2\n",
      "OYO_8FHENF2H23SK9LPK\n",
      "-----------------------------------------\n",
      "Accuracy P3S001: 0.7522123893805309\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexc\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4065 events found\n",
      "Event IDs: [ 1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "Not setting metadata\n",
      "4065 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 4065 events and 201 original time points ...\n",
      "2 bad epochs dropped\n",
      "4018 events found\n",
      "Event IDs: [1 2]\n",
      "Not setting metadata\n",
      "3337 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 3337 events and 201 original time points ...\n",
      "2 bad epochs dropped\n",
      "4018 events found\n",
      "Event IDs: [1 2]\n",
      "Not setting metadata\n",
      "681 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 681 events and 201 original time points ...\n",
      "0 bad epochs dropped\n",
      "4018 events found\n",
      "Event IDs: [1 2]\n",
      "Not setting metadata\n",
      "4018 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 4018 events and 201 original time points ...\n",
      "2 bad epochs dropped\n",
      "hlbls: 33\n",
      "hlbls: 33\n",
      "__4_44948__4_____9___9___________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexc\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Classification of Averaged Epochs\n",
      "____9___9___________\n",
      "3S_SJBRN4T8DCPCLI1P8\n",
      "-----------------------------------------\n",
      "Accuracy P3S002: 0.7743362831858407\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexc\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3963 events found\n",
      "Event IDs: [ 1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "Not setting metadata\n",
      "3963 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 3963 events and 201 original time points ...\n",
      "2 bad epochs dropped\n",
      "3894 events found\n",
      "Event IDs: [1 2]\n",
      "Not setting metadata\n",
      "3239 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 3239 events and 201 original time points ...\n",
      "2 bad epochs dropped\n",
      "3894 events found\n",
      "Event IDs: [1 2]\n",
      "Not setting metadata\n",
      "655 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 655 events and 201 original time points ...\n",
      "0 bad epochs dropped\n",
      "3894 events found\n",
      "Event IDs: [1 2]\n",
      "Not setting metadata\n",
      "3894 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 3894 events and 201 original time points ...\n",
      "2 bad epochs dropped\n",
      "hlbls: 32\n",
      "hlbls: 32\n",
      "___4___44__9_9___9__94____4_9___\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexc\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Classification of Averaged Epochs\n",
      "_9___9__94____4_9___\n",
      "P2T_97D8XD3CWQ5LZGEE\n",
      "-----------------------------------------\n",
      "Accuracy P3S003: 0.8127853881278538\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexc\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3985 events found\n",
      "Event IDs: [ 1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "Not setting metadata\n",
      "3985 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 3985 events and 201 original time points ...\n",
      "2 bad epochs dropped\n",
      "3896 events found\n",
      "Event IDs: [1 2]\n",
      "Not setting metadata\n",
      "3246 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 3246 events and 201 original time points ...\n",
      "2 bad epochs dropped\n",
      "3896 events found\n",
      "Event IDs: [1 2]\n",
      "Not setting metadata\n",
      "650 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 650 events and 201 original time points ...\n",
      "0 bad epochs dropped\n",
      "3896 events found\n",
      "Event IDs: [1 2]\n",
      "Not setting metadata\n",
      "3896 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 3896 events and 201 original time points ...\n",
      "2 bad epochs dropped\n",
      "hlbls: 32\n",
      "hlbls: 32\n",
      "_9____4____9___9___________9_9__\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexc\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Classification of Averaged Epochs\n",
      "___9___________9_9__\n",
      "YAH_9P5KVY4OS6XWMQ19\n",
      "-----------------------------------------\n",
      "Accuracy P3S004: 0.7808219178082192\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexc\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "#-----------------------------------------------------------------------\n",
    "# Librerías y algunas variables\n",
    "#-----------------------------------------------------------------------\n",
    "import mne                                                              # pip install mne\n",
    "mne.set_log_level('WARNING')                                            # Luego averiguar ¿para qué?\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb      \n",
    "import random                                              # Por ahora lo voy a usar para cambiar los nombres de las columnas\n",
    "\n",
    "import a_funciones as a_fun  \n",
    "\n",
    "color = ['green', 'blue','red','cyan', 'magenta', 'yellow','k','w']     # Paleta de colores para diferenciar las ondas\n",
    "xlabel = 'Muestra'\n",
    "ylabel = 'Amplitud (uV)'\n",
    "\n",
    "vector_desfase = [1, 2, 3, 4, 5]\n",
    "# vector_desfase = [5, 10, 15, 20, 25, 30]\n",
    "# vector_desfase = [20, 30, 40, 50, 60, 70]\n",
    "# vector_desfase = [2000, 2000, 2000, 2000, 2000, 2000]\n",
    "#vector_desfase = [0]                                     # Para graficar la señal sin alteraciones en fase, como referencia.\n",
    "print(f'vector_desfase: ',vector_desfase)\n",
    "\n",
    "def DrugSignal(signal, t_flash):\n",
    "    for j in range(0,7):\n",
    "        desfase = random.choice(vector_desfase)\n",
    "        for i in range(0,4200):\n",
    "            if (t_flash[i,3]==2):\n",
    "                #signal[t_flash[i,0]-1:t_flash[i,0]+250-1,:] += (erptemplate1*3)    ORIGINAL\n",
    "                signal[t_flash[i,0]-desfase:t_flash[i,0]+250-desfase,j] += (erptemplate1[:,j])                \n",
    "    return signal\n",
    "\n",
    "# Now load the basal EEG stream\n",
    "# mat = scipy.io.loadmat('./dataset/p300-subject-25.mat')\n",
    "# mat = scipy.io.loadmat('./dataset/itba/P300S02.mat')\n",
    "\n",
    "files_path_itba = ['./dataset/itba/P300S01.mat', './dataset/itba/P300S02.mat', \n",
    "                    './dataset/itba/P300S03.mat', './dataset/itba/P300S06.mat']\n",
    "# files_path_itba = ['./dataset/p300-subject-26.mat', './dataset/p300-subject-25.mat', \n",
    "#                  './dataset/p300-subject-25.mat', './dataset/p300-subject-26.mat']\n",
    "\n",
    "#                    './dataset/itba/P300S05.mat', './dataset/itba/P300S06.mat',\n",
    "#                    './dataset/itba/P300S07.mat', './dataset/itba/P300S08.mat']\n",
    "\n",
    "repetitions = 48\n",
    "\n",
    "for ii, path in enumerate(files_path_itba):  \n",
    "    mat = scipy.io.loadmat('./dataset/ERPTemplate.mat')\n",
    "    routput = mat['routput']\n",
    "    # In this ERPTemplate, there are two different template signals that are good.\n",
    "    erptemplate1 = routput[0][7][0][1][0][0][0][7] \n",
    "    # erptemplate2 = routput[0][7][0][1][0][0][0][0] \n",
    "\n",
    "    # The original ERPTemplate dataset has a sampling frequency of 256 so I need to perform a small downsampling to 250 Hz\n",
    "    erptemplate1 = np.delete( erptemplate1, range(0,256,43),0)\n",
    "    # erptemplate2 = np.delete( erptemplate2, range(0,256,43),0)\n",
    "    # erptemplate1 = np.load('./dataset/array_DrugAmpERPtemplate.npy')\n",
    "    # print(erptemplate1)\n",
    "    # print(erptemplate1.shape)\n",
    "    \n",
    "    '''\n",
    "    #-----------------------------------------------------------------------\n",
    "    # array_DrugAmpERPtemplate es un vector de 5 templates ERP con \n",
    "    # distintas variaciones en amplitud +/- 20% del valor max y min del ERPtemplate original\n",
    "    #-----------------------------------------------------------------------\n",
    "    v_min = erptemplate1.min()*20\n",
    "    v_max = erptemplate1.max()*20\n",
    "    array_DrugAmpERPtemplate = np.empty(5,dtype=object)\n",
    "    for i in range(5):\n",
    "        v_aleatorio = np.random.uniform(v_min, v_max, (1, 8)) \n",
    "        #print(f'v_aleatorio({i}): ',v_aleatorio)\n",
    "        # v_aleatorio = [-5,1,5,0,5,0,5,0] # Vector fijo para testing\n",
    "        DrugAmpERPtemplate = np.empty_like(erptemplate1) # Inicializo un array igual que erptemplate1\n",
    "        for j in range(erptemplate1.shape[0]):\n",
    "            DrugAmpERPtemplate[j, :] = erptemplate1[j, :] + v_aleatorio\n",
    "        array_DrugAmpERPtemplate[i] = DrugAmpERPtemplate\n",
    "        #print(f'array_DrugAmpERPtemplate[{i}].min()', array_DrugAmpERPtemplate[i].min())\n",
    "        #print(f'array_DrugAmpERPtemplate[{i}].max()', array_DrugAmpERPtemplate[i].max())\n",
    "        #print('----------------------------------------------------')\n",
    "\n",
    "    # erptemplate1 = array_DrugAmpERPtemplate[1]    \n",
    "    erptemplate1 = array_DrugAmpERPtemplate[0]\n",
    "    '''\n",
    "    results_mne=f'./a_results/20240201_fase.csv' # Si necesitás, con cambiar la extensión a.txt es suficiente\n",
    "    file_temp=open(results_mne,\"a\") \n",
    "    # print(f'P{ii+1},', file=file_temp) # COLUMNA0.\n",
    "    punto_mat=scipy.io.loadmat(path)\n",
    "\n",
    "    # Data point zero for the eight channels.  Should be in V.\n",
    "    signal = punto_mat['data'][0][0][0] \n",
    "    #* pow(10,6)\n",
    "\n",
    "    # Trials\n",
    "    t_trials = punto_mat['data'][0][0][3]\n",
    "\n",
    "    # Flash matrix\n",
    "    t_flash = punto_mat['data'][0][0][4]\n",
    "\n",
    "    signal = DrugSignal(signal, t_flash)\n",
    "\n",
    "    t_stim = punto_mat['data'][0][0][2]\n",
    "    t_type = punto_mat['data'][0][0][1]\n",
    "\n",
    "    ch_names=[ 'Fz'  ,  'Cz',    'P3' ,   'Pz'  ,  'P4'  ,  'PO7'   , 'PO8'   , 'Oz']\n",
    "    ch_types= ['eeg'] * signal.shape[1]\n",
    "\n",
    "    ch_names_events = ch_names + ['t_stim']+ ['t_type']\n",
    "    ch_types_events = ch_types + ['misc'] + ['misc']\n",
    "\n",
    "    #info = mne.create_info(ch_names, 250, ch_types=ch_types)\n",
    "    #eeg_mne = mne.io.array.RawArray(signal.T, info)\n",
    "\n",
    "    signal_events = np.concatenate([signal, t_stim, t_type],1)\n",
    "    info_events = mne.create_info(ch_names_events,250, ch_types_events)\n",
    "    eeg = mne.io.RawArray(signal_events.T, info_events)\n",
    "\n",
    "    # Do some basic signal processing (1-20 band pass filter)\n",
    "    # fig=eeg.plot_psd()\n",
    "    eeg.filter(1,20)\n",
    "    # fig=eeg.plot_psd()\n",
    "    event_times = mne.find_events(eeg, stim_channel='t_type')    \n",
    "    # eeg.plot(scalings='auto',n_channels=8,events=event_times,block=True)   # scalings=10e-05\n",
    "\n",
    "    if (np.unique(t_flash[:,0]).shape[0] != 4200):\n",
    "        u,c = np.unique( t_flash[:,0], return_counts=True)\n",
    "        dup = u[c>1]\n",
    "        for i in range(dup.shape[0]):\n",
    "            idx = np.where( t_flash[:,0] == dup[i] )[0][0]\n",
    "            t_flash[idx,0]  -= 1\n",
    "            t_flash[idx,1]  = 1\n",
    "            t_type[t_flash[idx,0]] = t_flash[idx,3]\n",
    "            t_stim[t_flash[idx,0]] = t_flash[idx,2]\n",
    "\n",
    "    np.unique(t_flash[:,0]).shape\n",
    "    assert  np.unique(t_flash[:,0]).shape[0] == 4200, 'Problem with experiment structure.  There aren''t enough events.'\n",
    "\n",
    "    def getstims(eeg_mne, eeg_events):\n",
    "        # Get the stimulations.  These are the FLASHINGS of rows and columns.\n",
    "        tmin = 0\n",
    "        tmax = 0.8\n",
    "        reject = None\n",
    "        event_times = mne.find_events(eeg_events, stim_channel='t_stim',shortest_event=0, verbose=True, min_duration=0.000001, consecutive=True)\n",
    "        event_id = {'Row1':1,'Row2':2,'Row3':3,'Row4':4,'Row5':5,'Row6':6,'Col1':7,'Col2':8,'Col3':9,'Col4':10,'Col5':11,'Col6':12}\n",
    "\n",
    "        epochs = mne.Epochs(eeg_mne, event_times, event_id, tmin, tmax, proj=False,\n",
    "                        baseline=None, reject=reject, preload=True,\n",
    "                        verbose=True, reject_by_annotation=None)\n",
    "\n",
    "        stims = event_times[:,-1]\n",
    "        return [epochs,stims]\n",
    "\n",
    "    stimepochs, stims = getstims(eeg, eeg)\n",
    "\n",
    "    def getlabels(eeg_mne, eeg_events, event_id):\n",
    "        # Get the hit/no hits labels.  \n",
    "        # These are the FLASHINGS of rows and columns but selected if they are the ones that will trigger the P300 response or not.\n",
    "        # event_id = { 'first':1, 'second':2 }\n",
    "        # baseline = (0.0, 0.2)\n",
    "        # reject = {'eeg': 70 * pow(10,6)}\n",
    "        tmin = 0\n",
    "        tmax = 0.8\n",
    "        reject = None\n",
    "        event_times = mne.find_events(eeg_events, stim_channel='t_type', shortest_event=0, verbose=True, min_duration=0.000001, consecutive=True)\n",
    "        epochs = mne.Epochs(eeg_mne, event_times, event_id, tmin, tmax, proj=False,\n",
    "                        baseline=None, reject=reject, preload=True,\n",
    "                        verbose=True, reject_by_annotation=None)\n",
    "        labels = epochs.events[:, -1]\n",
    "        return [epochs, labels]\n",
    "\n",
    "    epochs, labels = getlabels(eeg, eeg, {'first':1})\n",
    "    epocked = epochs.average()\n",
    "    # epocked.plot(window_title='NoHit Averaged Signals')\n",
    "    epochs, labels = getlabels(eeg, eeg, {'second':2})\n",
    "    epocked = epochs.average()\n",
    "    # epocked.plot(window_title='Hit Averaged Signals')\n",
    "    epochs, labels = getlabels(eeg, eeg, { 'first':1, 'second':2})\n",
    "\n",
    "    # Downsample the original FS=250 Hz signal to >>> 20 Hz\n",
    "    #epochs.resample(20, npad=\"auto\")\n",
    "    #stimepochs.resample(20, npad=\"auto\")\n",
    "    # repetitions = 120\n",
    "    # Con 100: El clasificador da resultados distintos.\n",
    "    # %%\n",
    "    # This is Single Flashing Classification attempt.\n",
    "    from sklearn.preprocessing import  StandardScaler\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn import svm\n",
    "    from sklearn.pipeline import make_pipeline\n",
    "\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "    # import a linear classifier from mne.decoding\n",
    "    from mne.decoding import LinearModel\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    clf = LogisticRegression(solver='lbfgs')\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # create a linear model with LogisticRegression\n",
    "    model = LinearModel(clf)\n",
    "\n",
    "    # Get the epoched data (get only the data columns)\n",
    "    eeg_data = epochs.get_data().reshape(len(labels), -1)\n",
    "    eeg_data = eeg_data[:,0:epochs.get_data().shape[2]*1]\n",
    "    #eeg_data[labels==2] = erptemplate1[:201,0]\n",
    "    #eeg_data[labels==1] = erptemplate1[:201,0]\n",
    "    #eeg_data[labels==2] = np.zeros((eeg_data.shape[1],))\n",
    "    #eeg_data[labels==1] = np.ones((eeg_data.shape[1],))\n",
    "    #labels = np.random.permutation(labels)\n",
    "\n",
    "    # fit the classifier on MEG data\n",
    "    X = scaler.fit_transform(eeg_data)\n",
    "\n",
    "    model.fit(X[0:2800], labels[0:2800])\n",
    "\n",
    "    preds = model.predict(X[2800:])\n",
    "\n",
    "    # Classification report\n",
    "    target_names = ['nohit', 'hit']\n",
    "\n",
    "    report = classification_report(labels[2800:], preds, target_names=target_names)\n",
    "    # print(report)\n",
    "\n",
    "    cm = confusion_matrix(labels[2800:], preds)\n",
    "    # print (cm)\n",
    "    cm_normalized = cm.astype(float) / cm.sum(axis=1)[:, np.newaxis]\n",
    "    acc=(cm[0,0]+cm[1,1])*1.0/(np.sum(cm))\n",
    "\n",
    "    '''\n",
    "    import matplotlib.pyplot as plt\n",
    "    for i in range(200,220):\n",
    "        plt.figure(figsize=(9, 3))\n",
    "        plt.plot(eeg_data[labels==1][i])\n",
    "        plt.show()\n",
    "    '''\n",
    "\n",
    "    # In[1]:   Average classification x trial (unbalanced)\n",
    "    # Este dataset es un dataset de calibración. El registro de EEG corresponde a un experimento del Speller\n",
    "    # de 35 letras (7 palabras de 5 letras).   Cada una de las letras consiste en 10 repeticiones de la intensificación \n",
    "    # de los 12 estímulos distintos, siendo cada estímulo el FLASHING de una de las 6 filas o 6 columnas.\n",
    "    # Cada vez que se repite, se hace una permutación de los 12.\n",
    "    # En cada una de esos 12 estímulos, dos, uno correspondiente a una fila y a una columna, corresponden \n",
    "    # a la letra que la persona está prestando atención y la idea es que el sistema descubra que letra es.\n",
    "\n",
    "    # Primero tengo que agarrar la lista de labels y asignar a los 420 (35x12)\n",
    "    # el label que le corresponde a cada uno.  Es decir de los primeros 12, 10\n",
    "    # son no hits y 2 hits.\n",
    "\n",
    "    # hlbls tiene pares (r,c) que representan la fila y la columna donde está la letra\n",
    "    # que la persona tiene que elegir. \n",
    "    hlbls = []\n",
    "    hpreds = []\n",
    "    classlabels=np.asarray([])\n",
    "\n",
    "    # ==================================================\n",
    "    redondeo_hlbls = int((abs(min(len(stims), len(labels))))/120)\n",
    "    print(\"hlbls:\", redondeo_hlbls)\n",
    "    # Este cálculo se hace porque no todos los resultados son iguales:\n",
    "    # Paciente\t            hlbls\tEventos\n",
    "    # P300S01.mat\t        33\t    396\n",
    "    # P300S02.mat\t        33\t    396\n",
    "    # P300S03.mat\t        32\t    384\n",
    "    # P300S04.mat\t        32\t    384\n",
    "    # P300S05.mat\t        33\t    396\n",
    "    # P300S06.mat\t        32\t    384\n",
    "    # P300S07.mat\t        32\t    384\n",
    "    # P300S08.mat\t        32\t    384\n",
    "    #p300-subject-25.mat\t35\t    420\n",
    "    # ==================================================\n",
    "\n",
    "    for trial in range(0,redondeo_hlbls):\n",
    "        a=np.zeros((12*10,2))\n",
    "        a[:,0] = stims[0+120*trial:0+120*trial+120]\n",
    "        a[:,1] = labels[0+120*trial:0+120*trial+120]    \n",
    "        \n",
    "        # b=np.zeros((12,1))\n",
    "        b=np.zeros((12,2))\n",
    "        \n",
    "        for i in range(1,13):\n",
    "            # print(\"valores únicos de a (stims):\", np.unique(a[a[:,0]==i,1]))\n",
    "            # print(\"a[:,0]:\", a[:,0])\n",
    "            # print(\"b[]:\", b[i-1])\n",
    "            b[i-1] = np.unique(a[a[:,0]==i,1])\n",
    "            # b[i-1] = list(np.unique(a[a[:,0]==i,1]))\n",
    "\n",
    "        b = b[:,1]\n",
    "        \n",
    "        for i in range(0,6):\n",
    "            if (b[i]==2):\n",
    "                r = i+1\n",
    "\n",
    "        for i in range(6,12):\n",
    "            if (b[i]==2):\n",
    "                c = i+1\n",
    "    \n",
    "        classlabels = np.append( classlabels, b )\n",
    "        assert (r!=0 and c!=0), 'Error %d,%d' % (r,c) \n",
    "        hlbls.append( (r,c) )\n",
    "    print(\"hlbls:\", len(hlbls))\n",
    "    #print(\"len(classlabels):\", len(classlabels))\n",
    "    # print(\"classlabels:\", classlabels)\n",
    "\n",
    "    def SpellMeLetter(row, col):\n",
    "        spellermatrix = [ ['A','B','C','D','E','F'],\n",
    "                        [ 'G','H','I','J','K','L'],\n",
    "                    [ 'M','N','O','P','Q','R'],\n",
    "                    [ 'S','T','U','V','W','X'],\n",
    "                    [ 'Y','Z','1','2','3','4'],\n",
    "                    [ '5','6','7','8','9','_'] ]\n",
    "\n",
    "        return spellermatrix[row-1][col-1-6]\n",
    "\n",
    "    # print(\"Esta es la frase de 7 palabras de 5 letras que la persona tiene que producir.\") \n",
    "    for i in range(0,redondeo_hlbls):\n",
    "        print(SpellMeLetter(hlbls[i][0],hlbls[i][1]),end='')    \n",
    "    print()\n",
    "\n",
    "    # Luego necesito calcular los 420 averaging (de repetitions)\n",
    "    # Finalmente aprendo con 180 y me fijo si predigo los 240\n",
    "    # De los 240 adivino 20 letras (de a pares) y con eso calculo la performance\n",
    "\n",
    "    def getaverageepoch(singleepoch):\n",
    "        # Build the epochs based on each stimulation (1-12), and put all the epochs togheter.\n",
    "        # Construye las épocas/eventos en función de cada estimulación (1-12) y junte todas las épocas/eventos.\n",
    "        \n",
    "        for trial in range(0,redondeo_hlbls):\n",
    "            epochstrial = singleepoch[0+repetitions*trial:repetitions*trial+repetitions]\n",
    "            '''\n",
    "            print(\"epochr1:\", epochr1)\n",
    "            print(\"epochr1.shape:\", epochr1.data().shape)\n",
    "            print(\"epochr1:\", epochr1.get_data())\n",
    "            print(\"epochr1.get_data.shape:\", epochr1.get_data().shape)\n",
    "            print(\"epochr1.average.data.shape:\", [epochr1.average().data.shape])\n",
    "            #print(\"epochs_data\", epochs_data)\n",
    "            '''\n",
    "            epochr1 = epochstrial['Row1'] # print(\"epochr1:\", epochr1) # print(\"epochstrial['Row1']:\", epochstrial['Row1'])\n",
    "            epochr2 = epochstrial['Row2'] # print(\"epochstrial['Row2']:\", epochstrial['Row2'])\n",
    "            epochr3 = epochstrial['Row3']\n",
    "            epochr4 = epochstrial['Row4']\n",
    "            epochr5 = epochstrial['Row5']\n",
    "            epochr6 = epochstrial['Row6']\n",
    "            \n",
    "            epochc1 = epochstrial['Col1']\n",
    "            epochc2 = epochstrial['Col2']\n",
    "            epochc3 = epochstrial['Col3']\n",
    "            epochc4 = epochstrial['Col4']\n",
    "            epochc5 = epochstrial['Col5']\n",
    "            epochc6 = epochstrial['Col6']\n",
    "            \n",
    "            if (trial==0):\n",
    "                epochs_data = np.array([epochr1.average().data])\n",
    "                #print(\"epochs_data.shape:\", epochs_data.shape)\n",
    "            else:\n",
    "                epochs_data = np.concatenate((epochs_data, [epochr1.average().data]), axis=0)\n",
    "                #print(\"epochr1.average().data.shape:\", epochr1.average().data.shape)\n",
    "            epochs_data = np.concatenate((epochs_data, [epochr2.average().data]), axis=0)\n",
    "            epochs_data = np.concatenate((epochs_data, [epochr3.average().data]), axis=0)\n",
    "            epochs_data = np.concatenate((epochs_data, [epochr4.average().data]), axis=0)\n",
    "            epochs_data = np.concatenate((epochs_data, [epochr5.average().data]), axis=0)\n",
    "            epochs_data = np.concatenate((epochs_data, [epochr6.average().data]), axis=0)\n",
    "\n",
    "            epochs_data = np.concatenate((epochs_data, [epochc1.average().data]), axis=0)\n",
    "            epochs_data = np.concatenate((epochs_data, [epochc2.average().data]), axis=0)\n",
    "            epochs_data = np.concatenate((epochs_data, [epochc3.average().data]), axis=0)\n",
    "            epochs_data = np.concatenate((epochs_data, [epochc4.average().data]), axis=0)\n",
    "            epochs_data = np.concatenate((epochs_data, [epochc5.average().data]), axis=0)\n",
    "            epochs_data = np.concatenate((epochs_data, [epochc6.average().data]), axis=0)\n",
    "\n",
    "            # print(f'epochs_data: ', epochs_data)\n",
    "\n",
    "        # There are 420 epochs, which correspond to 35 letters, in groups of 12.\n",
    "        events=np.array([np.arange(len(classlabels)),np.zeros(len(classlabels)), classlabels])\n",
    "        events = events.T\n",
    "        events = events.astype(int)\n",
    "                        \n",
    "        tmin = 0\n",
    "        tmax = 0.8\n",
    "        event_id = { 'first':1, 'second':2 }\n",
    "        info = mne.create_info(ch_names, 250, ch_types=ch_types)\n",
    "        custom_epochs = mne.EpochsArray(epochs_data, info, events, tmin, event_id) \n",
    "\n",
    "        return custom_epochs\n",
    "\n",
    "    # avepochs contains all the 420 averaged epochs, 7 letters of 5, with 12 each.\n",
    "    custom_epochs = getaverageepoch(stimepochs)          \n",
    "\n",
    "    # Performs the final classification, the one that allows to produce the spelled letters.\n",
    "    print('Performance Classification of Averaged Epochs')\n",
    "    clf = LogisticRegression(solver='lbfgs')\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # create a linear model with LogisticRegression\n",
    "    model = LinearModel(clf)\n",
    "\n",
    "    # =============================================\n",
    "    # Acá redimensiono el dataset para que training y test queden \n",
    "    # en función de la cantidad de eventos reales encontrados.\n",
    "    # no 420 exactos.\n",
    "    eventos_encontrados = len(custom_epochs)  \n",
    "    long_training = int(abs(len(custom_epochs))*(0.43))\n",
    "    long_test = ((len(custom_epochs))-long_training)\n",
    "    #redondeo_hlbls\n",
    "    redondeo_hlbls_40 = int(abs(redondeo_hlbls*(0.4)))\n",
    "    #print(\"redondeo_hlbls_40:\", redondeo_hlbls_40)\n",
    "    # =============================================\n",
    "\n",
    "    # print(\"len(eventos_encontrados):\", len(custom_epochs))\n",
    "    # print(\"long_training:\", long_training)\n",
    "    # print(\"long_test:\", long_test)\n",
    "    # print(\"redondeo_hlbls:\", redondeo_hlbls)\n",
    "    # print(\"redondeo_hlbls_40:\", redondeo_hlbls_40)\n",
    "\n",
    "    # training = range(0,180)\n",
    "    # test = range(180,420)\n",
    "\n",
    "    training = range(0,long_training)\n",
    "    test = range(long_training,eventos_encontrados)\n",
    "\n",
    "    #print(\"training:\", training)\n",
    "    #print(\"test:\", test)\n",
    "\n",
    "    eeg_data = custom_epochs.get_data()\n",
    "\n",
    "    #print(\"Tipo (eeg_data) : \", type(eeg_data))\n",
    "    #print(\"len(eeg_data):\", len(eeg_data))\n",
    "    # print(\"shape(eeg_data):\", eeg_data.shape)\n",
    "    #print(\"eeg_data:\", eeg_data)\n",
    "    #np.savetxt(f'./a_results/eeg_data.csv', eeg_data, delimiter=',',fmt='%s')\n",
    "    # np.savetxt(f'./a_results/events2.csv', events, delimiter='\\t',fmt='%s')\n",
    "\n",
    "    eeg_data = eeg_data.reshape(eventos_encontrados, -1)\n",
    "\n",
    "    # print(\"re-shape(eeg_data):\", eeg_data.shape)\n",
    "\n",
    "    eeg_data = eeg_data[:,0:201]\n",
    "\n",
    "    #eeg_data[classlabels==2] = np.zeros((eeg_data.shape[1],))\n",
    "    #eeg_data[classlabels==1] = np.ones((eeg_data.shape[1],))\n",
    "\n",
    "    #X = scaler.fit_transform(eeg_data)\n",
    "\n",
    "    X = eeg_data\n",
    "\n",
    "    cf = clf.fit(X[training], classlabels[training])\n",
    "\n",
    "    classpreds = np.empty ((eventos_encontrados,2))\n",
    "\n",
    "    classpreds[test,:] = clf.predict_proba(X[test])\n",
    "\n",
    "    hpreds = []\n",
    "\n",
    "    for trial in range(redondeo_hlbls_40,redondeo_hlbls):\n",
    "    #for trial in range(15,35):\n",
    "        #print('Row')\n",
    "        for i in range(0,6):\n",
    "            preds = classpreds[trial*12+i]\n",
    "            #print ( preds[1] )\n",
    "            labels = classlabels[trial*12+i]\n",
    "\n",
    "        #print (  np.argmin( classpreds[trial*12+0:trial*12+6]))\n",
    "        r = np.argmax( classpreds[trial*12+0:trial*12+6,1])+1\n",
    "        \n",
    "        #print('Col')\n",
    "        for i in range(6,12):\n",
    "            preds = classpreds[trial*12+i]\n",
    "            #print ( preds[1] )\n",
    "            labels = classlabels[trial*12+i]\n",
    "\n",
    "        #print (  np.argmin( classpreds[trial*12+6:trial*12+12]))\n",
    "        c = np.argmax( classpreds[trial*12+6:trial*12+12,1])+1\n",
    "\n",
    "        hpreds.append( (r,c) )\n",
    "\n",
    "    # print(\"lo esperado:\")\n",
    "    for i in range(redondeo_hlbls_40,redondeo_hlbls):\n",
    "    #for i in range(15,35):\n",
    "        print(SpellMeLetter(hlbls[i][0],hlbls[i][1]),end='')\n",
    "    print()\n",
    "\n",
    "    # print(\"lo predecido\")\n",
    "    for i in range(redondeo_hlbls_40,redondeo_hlbls):\n",
    "    #for i in range(15,35):\n",
    "        print(SpellMeLetter(hpreds[i-15][0],hpreds[i-15][1]),end='')\n",
    "    print()\n",
    "\n",
    "    '''\n",
    "    # %%\n",
    "    for i in range(0,12):\n",
    "        plt.figure(figsize=(9, 3))\n",
    "        plt.plot(eeg_data[i])\n",
    "        plt.show()\n",
    "    # %%\n",
    "    '''\n",
    "\n",
    "    # Classification report\n",
    "    target_names = ['nohit', 'hit']\n",
    "    report = classification_report(classlabels[test], clf.predict(X[test]), target_names=target_names)\n",
    "    # print(report)\n",
    "    # print(f'report: ', report)\n",
    "    cm = confusion_matrix(classlabels[test], clf.predict(X[test]) )\n",
    "    # print (cm)\n",
    "    cm_normalized = cm.astype(float) / cm.sum(axis=1)[:, np.newaxis]\n",
    "    acc=(cm[0,0]+cm[1,1])*1.0/(np.sum(cm))\n",
    "    # print(\"cm:\", cm)\n",
    "    print(\"-----------------------------------------\")  \n",
    "    print(f'Accuracy P3S00{ii+1}: {acc}') # COLUMNA0.\n",
    "    print(f'P{ii+1}, {acc-0.2}, {repetitions}', file=file_temp)\n",
    "    print(\"-----------------------------------------\")  \n",
    "\n",
    "file_temp.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "f8696d3d898bc62b90f4cd7d878fc87a051ec77bbff0b42338ab50969b9d3714"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
