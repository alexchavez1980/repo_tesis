{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne                                                              # Librería de python para explorar, visualizar,\n",
    "mne.set_log_level('WARNING')                                            # y analizar datos neurofisiológicos humanos.\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb                                                    \n",
    "color = ['green', 'blue','red','cyan', 'magenta', 'yellow','k','w']     # Paleta de colores para diferenciar las ondas\n",
    "\n",
    "import a_funciones as a_fun                                             # Funciones Alex\n",
    "import os                                                               # Para importar varios archivos\n",
    "\n",
    "xlabel = 'Muestra'                                                      # Abscisas\n",
    "ylabel = 'Amplitud (uV)'                                                # Ordenadas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a_Lag&DrugSignal_v4.ipynb.\n",
    "##### Recorro un grupo de señales EEG con variantes en amplitud y fase para detectar si hay hit/nohits.  \n",
    "##### Alteración del DrugSignal para probar si obtengo resultados distintos.\n",
    "##### Comienzo a meter los modelos de ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nnp.savetxt(f'./a_results/meta_P300S02.csv', meta_P300S02, delimiter=',',fmt='%s')\\nfor j, cont_P300S01 in enumerate(meta_P300S01):   \\n    df = a_fun.to_df(cont_P300S01)\\n    titulo = (f'P300S01 con un desfase de {lag_flash[j]}')\\n    a_fun.grafic_8ch_test(df, titulo, xlabel, ylabel)  \\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carga y ajuste de la plantilla de un ERP seleccionado previamente.\n",
    "mat = scipy.io.loadmat('./dataset/ERPTemplate.mat')\n",
    "routput = mat['routput']\n",
    "erptemplate1 = routput[0][7][0][1][0][0][0][7]                              # erptemplate2 = routput[0][7][0][1][0][0][0][0] \n",
    "erptemplate1 = np.delete( erptemplate1, range(0,256,43),0)                  # erptemplate2 = np.delete( erptemplate2, range(0,256,43),0)\n",
    "\n",
    "#-------------------------------------\n",
    "# Observación general del DS ITBA: Pacientes experimento PASIVO -> P300S01,02,03,06. | Pacientes experimento ACTIVO: P300S04, 05, 07 y 08.\n",
    "# NOTA IMPORTANTE: SI DROGÁS LA SEÑAL ES PORQUE ESTÁS USANDO UNA TRAZA EEG DE PACIENTES EN MODALIDAD PASIVA.\n",
    "# Es decir, que no se están enfocando en nada particular. Con éstos EEG + ERP se crea el dataset sintético.\n",
    "#-------------------------------------\n",
    "\n",
    "# Carga de la señal EEG\n",
    "punto_mat = scipy.io.loadmat('./dataset/itba/P300S01.mat') \n",
    "signal = punto_mat['data'][0][0][0] \n",
    "\n",
    "'''\n",
    "df_signal = a_fun.to_df(signal)\n",
    "titulo = 'signal original'\n",
    "#a_fun.grafic_8ch_test(df_signal, titulo, xlabel, ylabel)  \n",
    "'''\n",
    "\n",
    "#t_trials = punto_mat['data'][0][0][3]\n",
    "t_flash = punto_mat['data'][0][0][4]\n",
    "#t_stim = mat['data'][0][0][2]\n",
    "#t_type = mat['data'][0][0][1]\n",
    "\n",
    "# Vector principal donde se definen los desfases que tendrá la señal principal EEG.\n",
    "# lag_flash = [1, 2, 3, 4, 5, 10, 11, 12, 13, 14, 15]\n",
    "# lag_flash = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
    "lag_flash = [1, 2, 3, 4, 5]\n",
    "\n",
    "# Genero un conjunto de archivos, cada uno de ellos es una variación de la señal ppal EEG desfasada.\n",
    "meta_P300S01 = np.empty(len(lag_flash), dtype=object)\n",
    "for j,lag in enumerate(lag_flash):   \n",
    "    # print(f'Posición del vector lag_flash:{j} | Desfase de {lag} muestras')\n",
    "    punto_mat = scipy.io.loadmat('./dataset/itba/P300S01.mat')\n",
    "    signal = punto_mat['data'][0][0][0] \n",
    "    for i in range(0,4200):\n",
    "        if (t_flash[i,3]==2):\n",
    "            signal[t_flash[i,0]-lag:t_flash[i,0]+250-lag,:] += (erptemplate1*lag)\n",
    "    meta_P300S01[j] = signal\n",
    "    np.savetxt(f'./a_results/P300S01-lag{j}.csv', meta_P300S01[j], delimiter=',',fmt='%s')\n",
    "\n",
    "files_path_LagDrugSignal = ['./a_results/P300S01-lag0.csv', './a_results/P300S01-lag1.csv', './a_results/P300S01-lag2.csv',\n",
    "                            './a_results/P300S01-lag3.csv', './a_results/P300S01-lag4.csv']\n",
    "\n",
    "'''\n",
    "files_path_LagDrugSignal = ['./a_results/P300S01-lag0.csv', './a_results/P300S01-lag1.csv', './a_results/P300S01-lag2.csv',\n",
    "                            './a_results/P300S01-lag3.csv', './a_results/P300S01-lag4.csv', './a_results/P300S01-lag5.csv', \n",
    "                            './a_results/P300S01-lag6.csv', './a_results/P300S01-lag7.csv', './a_results/P300S01-lag8.csv', \n",
    "                            './a_results/P300S01-lag9.csv', './a_results/P300S01-lag10.csv']\n",
    "'''\n",
    "\n",
    "# Graficar todas las señales EEG desfasadas:\n",
    "'''\n",
    "for j, path in enumerate(files_path_LagDrugSignal):   \n",
    "    signal_grafica = np.loadtxt(path, delimiter=',')\n",
    "    df = a_fun.to_df(signal_grafica)\n",
    "    titulo = (f'P300S02 con un desfase de {lag_flash[j]} muestras')\n",
    "    a_fun.grafic_8ch_test(df, titulo, xlabel, ylabel)  \n",
    "'''\n",
    "\n",
    "# Almaceno toda la info de los distintos resultados en un mismo archivo para luego graficarlo:\n",
    "'''\n",
    "np.savetxt(f'./a_results/meta_P300S02.csv', meta_P300S02, delimiter=',',fmt='%s')\n",
    "for j, cont_P300S01 in enumerate(meta_P300S01):   \n",
    "    df = a_fun.to_df(cont_P300S01)\n",
    "    titulo = (f'P300S01 con un desfase de {lag_flash[j]}')\n",
    "    a_fun.grafic_8ch_test(df, titulo, xlabel, ylabel)  \n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4040 events found\n",
      "Event IDs: [ 1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "Not setting metadata\n",
      "4040 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 4040 events and 201 original time points ...\n",
      "0 bad epochs dropped\n",
      "\n",
      "-----------------------------------\n",
      "4004 events found\n",
      "Event IDs: [1 2]\n",
      "Not setting metadata\n",
      "3333 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 3333 events and 201 original time points ...\n",
      "0 bad epochs dropped\n",
      "\n",
      "-----------------------------------\n",
      "4004 events found\n",
      "Event IDs: [1 2]\n",
      "Not setting metadata\n",
      "671 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 671 events and 201 original time points ...\n",
      "0 bad epochs dropped\n",
      "\n",
      "-----------------------------------\n",
      "4004 events found\n",
      "Event IDs: [1 2]\n",
      "Not setting metadata\n",
      "4004 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 4004 events and 201 original time points ...\n",
      "0 bad epochs dropped\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       nohit       0.85      0.99      0.91      1000\n",
      "         hit       0.71      0.12      0.21       204\n",
      "\n",
      "    accuracy                           0.84      1204\n",
      "   macro avg       0.78      0.56      0.56      1204\n",
      "weighted avg       0.82      0.84      0.79      1204\n",
      "\n",
      "Reporte de clasificación:               precision    recall  f1-score   support\n",
      "\n",
      "       nohit       0.85      0.99      0.91      1000\n",
      "         hit       0.71      0.12      0.21       204\n",
      "\n",
      "    accuracy                           0.84      1204\n",
      "   macro avg       0.78      0.56      0.56      1204\n",
      "weighted avg       0.82      0.84      0.79      1204\n",
      "\n",
      "[[990  10]\n",
      " [179  25]]\n",
      "Matriz de confusión: [[990  10]\n",
      " [179  25]]\n",
      "[[0.99       0.01      ]\n",
      " [0.87745098 0.12254902]]\n",
      "Matriz de confusión normalizada: [[0.99       0.01      ]\n",
      " [0.87745098 0.12254902]]\n",
      "0.8430232558139535\n",
      "\"Accuracy\": 0.8430232558139535\n",
      "Cálculo de la precisión por separado, sin separar nohit de hit: 0.8468776732249786\n",
      "Cálculo de la precisión por separado, separando nohit de hit: 0.8468776732249786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexc\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4040 events found\n",
      "Event IDs: [ 1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "Not setting metadata\n",
      "4040 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 4040 events and 201 original time points ...\n",
      "0 bad epochs dropped\n",
      "\n",
      "-----------------------------------\n",
      "4004 events found\n",
      "Event IDs: [1 2]\n",
      "Not setting metadata\n",
      "3333 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 3333 events and 201 original time points ...\n",
      "0 bad epochs dropped\n",
      "\n",
      "-----------------------------------\n",
      "4004 events found\n",
      "Event IDs: [1 2]\n",
      "Not setting metadata\n",
      "671 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 671 events and 201 original time points ...\n",
      "0 bad epochs dropped\n",
      "\n",
      "-----------------------------------\n",
      "4004 events found\n",
      "Event IDs: [1 2]\n",
      "Not setting metadata\n",
      "4004 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 4004 events and 201 original time points ...\n",
      "0 bad epochs dropped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexc\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       nohit       0.91      0.96      0.94      1000\n",
      "         hit       0.75      0.52      0.62       204\n",
      "\n",
      "    accuracy                           0.89      1204\n",
      "   macro avg       0.83      0.74      0.78      1204\n",
      "weighted avg       0.88      0.89      0.88      1204\n",
      "\n",
      "Reporte de clasificación:               precision    recall  f1-score   support\n",
      "\n",
      "       nohit       0.91      0.96      0.94      1000\n",
      "         hit       0.75      0.52      0.62       204\n",
      "\n",
      "    accuracy                           0.89      1204\n",
      "   macro avg       0.83      0.74      0.78      1204\n",
      "weighted avg       0.88      0.89      0.88      1204\n",
      "\n",
      "[[965  35]\n",
      " [ 97 107]]\n",
      "Matriz de confusión: [[965  35]\n",
      " [ 97 107]]\n",
      "[[0.965     0.035    ]\n",
      " [0.4754902 0.5245098]]\n",
      "Matriz de confusión normalizada: [[0.965     0.035    ]\n",
      " [0.4754902 0.5245098]]\n",
      "0.8903654485049833\n",
      "\"Accuracy\": 0.8903654485049833\n",
      "Cálculo de la precisión por separado, sin separar nohit de hit: 0.908662900188324\n",
      "Cálculo de la precisión por separado, separando nohit de hit: 0.908662900188324\n",
      "4040 events found\n",
      "Event IDs: [ 1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "Not setting metadata\n",
      "4040 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 4040 events and 201 original time points ...\n",
      "0 bad epochs dropped\n",
      "\n",
      "-----------------------------------\n",
      "4004 events found\n",
      "Event IDs: [1 2]\n",
      "Not setting metadata\n",
      "3333 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 3333 events and 201 original time points ...\n",
      "0 bad epochs dropped\n",
      "\n",
      "-----------------------------------\n",
      "4004 events found\n",
      "Event IDs: [1 2]\n",
      "Not setting metadata\n",
      "671 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 671 events and 201 original time points ...\n",
      "0 bad epochs dropped\n",
      "\n",
      "-----------------------------------\n",
      "4004 events found\n",
      "Event IDs: [1 2]\n",
      "Not setting metadata\n",
      "4004 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 4004 events and 201 original time points ...\n",
      "0 bad epochs dropped\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       nohit       0.96      0.97      0.97      1000\n",
      "         hit       0.86      0.81      0.84       204\n",
      "\n",
      "    accuracy                           0.95      1204\n",
      "   macro avg       0.91      0.89      0.90      1204\n",
      "weighted avg       0.95      0.95      0.95      1204\n",
      "\n",
      "Reporte de clasificación:               precision    recall  f1-score   support\n",
      "\n",
      "       nohit       0.96      0.97      0.97      1000\n",
      "         hit       0.86      0.81      0.84       204\n",
      "\n",
      "    accuracy                           0.95      1204\n",
      "   macro avg       0.91      0.89      0.90      1204\n",
      "weighted avg       0.95      0.95      0.95      1204\n",
      "\n",
      "[[974  26]\n",
      " [ 38 166]]\n",
      "Matriz de confusión: [[974  26]\n",
      " [ 38 166]]\n",
      "[[0.974      0.026     ]\n",
      " [0.18627451 0.81372549]]\n",
      "Matriz de confusión normalizada: [[0.974      0.026     ]\n",
      " [0.18627451 0.81372549]]\n",
      "0.946843853820598\n",
      "\"Accuracy\": 0.946843853820598\n",
      "Cálculo de la precisión por separado, sin separar nohit de hit: 0.9624505928853755\n",
      "Cálculo de la precisión por separado, separando nohit de hit: 0.9624505928853755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexc\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4040 events found\n",
      "Event IDs: [ 1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "Not setting metadata\n",
      "4040 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 4040 events and 201 original time points ...\n",
      "0 bad epochs dropped\n",
      "\n",
      "-----------------------------------\n",
      "4004 events found\n",
      "Event IDs: [1 2]\n",
      "Not setting metadata\n",
      "3333 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 3333 events and 201 original time points ...\n",
      "0 bad epochs dropped\n",
      "\n",
      "-----------------------------------\n",
      "4004 events found\n",
      "Event IDs: [1 2]\n",
      "Not setting metadata\n",
      "671 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 671 events and 201 original time points ...\n",
      "0 bad epochs dropped\n",
      "\n",
      "-----------------------------------\n",
      "4004 events found\n",
      "Event IDs: [1 2]\n",
      "Not setting metadata\n",
      "4004 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 4004 events and 201 original time points ...\n",
      "0 bad epochs dropped\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       nohit       0.98      0.99      0.98      1000\n",
      "         hit       0.93      0.90      0.91       204\n",
      "\n",
      "    accuracy                           0.97      1204\n",
      "   macro avg       0.95      0.94      0.95      1204\n",
      "weighted avg       0.97      0.97      0.97      1204\n",
      "\n",
      "Reporte de clasificación:               precision    recall  f1-score   support\n",
      "\n",
      "       nohit       0.98      0.99      0.98      1000\n",
      "         hit       0.93      0.90      0.91       204\n",
      "\n",
      "    accuracy                           0.97      1204\n",
      "   macro avg       0.95      0.94      0.95      1204\n",
      "weighted avg       0.97      0.97      0.97      1204\n",
      "\n",
      "[[986  14]\n",
      " [ 21 183]]\n",
      "Matriz de confusión: [[986  14]\n",
      " [ 21 183]]\n",
      "[[0.986      0.014     ]\n",
      " [0.10294118 0.89705882]]\n",
      "Matriz de confusión normalizada: [[0.986      0.014     ]\n",
      " [0.10294118 0.89705882]]\n",
      "0.9709302325581395\n",
      "\"Accuracy\": 0.9709302325581395\n",
      "Cálculo de la precisión por separado, sin separar nohit de hit: 0.9791459781529295\n",
      "Cálculo de la precisión por separado, separando nohit de hit: 0.9791459781529295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexc\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4040 events found\n",
      "Event IDs: [ 1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "Not setting metadata\n",
      "4040 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 4040 events and 201 original time points ...\n",
      "0 bad epochs dropped\n",
      "\n",
      "-----------------------------------\n",
      "4004 events found\n",
      "Event IDs: [1 2]\n",
      "Not setting metadata\n",
      "3333 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 3333 events and 201 original time points ...\n",
      "0 bad epochs dropped\n",
      "\n",
      "-----------------------------------\n",
      "4004 events found\n",
      "Event IDs: [1 2]\n",
      "Not setting metadata\n",
      "671 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 671 events and 201 original time points ...\n",
      "0 bad epochs dropped\n",
      "\n",
      "-----------------------------------\n",
      "4004 events found\n",
      "Event IDs: [1 2]\n",
      "Not setting metadata\n",
      "4004 matching events found\n",
      "No baseline correction applied\n",
      "Using data from preloaded Raw for 4004 events and 201 original time points ...\n",
      "0 bad epochs dropped\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       nohit       0.99      0.99      0.99      1000\n",
      "         hit       0.94      0.95      0.94       204\n",
      "\n",
      "    accuracy                           0.98      1204\n",
      "   macro avg       0.96      0.97      0.97      1204\n",
      "weighted avg       0.98      0.98      0.98      1204\n",
      "\n",
      "Reporte de clasificación:               precision    recall  f1-score   support\n",
      "\n",
      "       nohit       0.99      0.99      0.99      1000\n",
      "         hit       0.94      0.95      0.94       204\n",
      "\n",
      "    accuracy                           0.98      1204\n",
      "   macro avg       0.96      0.97      0.97      1204\n",
      "weighted avg       0.98      0.98      0.98      1204\n",
      "\n",
      "[[987  13]\n",
      " [ 10 194]]\n",
      "Matriz de confusión: [[987  13]\n",
      " [ 10 194]]\n",
      "[[0.987      0.013     ]\n",
      " [0.04901961 0.95098039]]\n",
      "Matriz de confusión normalizada: [[0.987      0.013     ]\n",
      " [0.04901961 0.95098039]]\n",
      "0.9808970099667774\n",
      "\"Accuracy\": 0.9808970099667774\n",
      "Cálculo de la precisión por separado, sin separar nohit de hit: 0.9899699097291875\n",
      "Cálculo de la precisión por separado, separando nohit de hit: 0.9899699097291875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexc\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Resultados Paciente P300S01-lag0</th>\n",
       "      <th>Resultados Paciente P300S01-lag1</th>\n",
       "      <th>Resultados Paciente P300S01-lag2</th>\n",
       "      <th>Resultados Paciente P300S01-lag3</th>\n",
       "      <th>Resultados Paciente P300S01-lag4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Eventos coincidentes encontrados (sin tmin &amp; tmax)</th>\n",
       "      <td>3994</td>\n",
       "      <td>3994</td>\n",
       "      <td>3994</td>\n",
       "      <td>3994</td>\n",
       "      <td>3994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IDs de los eventos</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eventos coincidentes encontrados (con tmin:0 &amp; tmax:0.8)</th>\n",
       "      <td>4040</td>\n",
       "      <td>4040</td>\n",
       "      <td>4040</td>\n",
       "      <td>4040</td>\n",
       "      <td>4040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IDs de los eventos</th>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>getlabels Nohits -&gt; epochs</th>\n",
       "      <td>3333</td>\n",
       "      <td>3333</td>\n",
       "      <td>3333</td>\n",
       "      <td>3333</td>\n",
       "      <td>3333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>getlabels Nohits -&gt; Time</th>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>getlabels Hits -&gt; epochs</th>\n",
       "      <td>671</td>\n",
       "      <td>671</td>\n",
       "      <td>671</td>\n",
       "      <td>671</td>\n",
       "      <td>671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>getlabels Hits -&gt; Time</th>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Resultados Paciente P300S01-lag0  \\\n",
       "Eventos coincidentes encontrados (sin tmin & tmax)                              3994   \n",
       "IDs de los eventos                                                                 2   \n",
       "Eventos coincidentes encontrados (con tmin:0 & ...                              4040   \n",
       "IDs de los eventos                                                                12   \n",
       "getlabels Nohits -> epochs                                                      3333   \n",
       "getlabels Nohits -> Time                                                         201   \n",
       "getlabels Hits -> epochs                                                         671   \n",
       "getlabels Hits -> Time                                                           201   \n",
       "\n",
       "                                                    Resultados Paciente P300S01-lag1  \\\n",
       "Eventos coincidentes encontrados (sin tmin & tmax)                              3994   \n",
       "IDs de los eventos                                                                 2   \n",
       "Eventos coincidentes encontrados (con tmin:0 & ...                              4040   \n",
       "IDs de los eventos                                                                12   \n",
       "getlabels Nohits -> epochs                                                      3333   \n",
       "getlabels Nohits -> Time                                                         201   \n",
       "getlabels Hits -> epochs                                                         671   \n",
       "getlabels Hits -> Time                                                           201   \n",
       "\n",
       "                                                    Resultados Paciente P300S01-lag2  \\\n",
       "Eventos coincidentes encontrados (sin tmin & tmax)                              3994   \n",
       "IDs de los eventos                                                                 2   \n",
       "Eventos coincidentes encontrados (con tmin:0 & ...                              4040   \n",
       "IDs de los eventos                                                                12   \n",
       "getlabels Nohits -> epochs                                                      3333   \n",
       "getlabels Nohits -> Time                                                         201   \n",
       "getlabels Hits -> epochs                                                         671   \n",
       "getlabels Hits -> Time                                                           201   \n",
       "\n",
       "                                                    Resultados Paciente P300S01-lag3  \\\n",
       "Eventos coincidentes encontrados (sin tmin & tmax)                              3994   \n",
       "IDs de los eventos                                                                 2   \n",
       "Eventos coincidentes encontrados (con tmin:0 & ...                              4040   \n",
       "IDs de los eventos                                                                12   \n",
       "getlabels Nohits -> epochs                                                      3333   \n",
       "getlabels Nohits -> Time                                                         201   \n",
       "getlabels Hits -> epochs                                                         671   \n",
       "getlabels Hits -> Time                                                           201   \n",
       "\n",
       "                                                    Resultados Paciente P300S01-lag4  \n",
       "Eventos coincidentes encontrados (sin tmin & tmax)                              3994  \n",
       "IDs de los eventos                                                                 2  \n",
       "Eventos coincidentes encontrados (con tmin:0 & ...                              4040  \n",
       "IDs de los eventos                                                                12  \n",
       "getlabels Nohits -> epochs                                                      3333  \n",
       "getlabels Nohits -> Time                                                         201  \n",
       "getlabels Hits -> epochs                                                         671  \n",
       "getlabels Hits -> Time                                                           201  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CICLO PRINCIPAL #1: \n",
    "# Tomo los EEGs con el el ERP adicionado y cada uno con un desfase, para luego encontrar los hits/no hits en cada uno de ellos. \n",
    "for ii, path in enumerate(files_path_LagDrugSignal):   \n",
    "    #-------------------------------------\n",
    "    results_mne=f'./a_results/Resultados Paciente P300S01-lag{ii}.csv' # Se puede cambiar a .txt pero NO disminuye el tamaño del archivo.\n",
    "    file_temp=open(results_mne,\"w\") \n",
    "    print(f', Paciente meta_P300S01-lag{ii}', file=file_temp) # COLUMNA0.\n",
    "    punto_mat = scipy.io.loadmat('./dataset/itba/P300S01.mat')\n",
    "    signal = np.loadtxt(path, delimiter=',') \n",
    "    t_trials = punto_mat['data'][0][0][3]\n",
    "    t_flash = punto_mat['data'][0][0][4]\n",
    "    t_stim = punto_mat['data'][0][0][2]\n",
    "    t_type = punto_mat['data'][0][0][1]\n",
    "    #-------------------------------------\n",
    "    ch_names=[ 'Fz'  ,  'Cz',    'P3' ,   'Pz'  ,  'P4'  ,  'PO7'   , 'PO8'   , 'Oz']\n",
    "    ch_types= ['eeg'] * signal.shape[1]\n",
    "    ch_names_events = ch_names + ['t_stim']+ ['t_type']\n",
    "    ch_types_events = ch_types + ['misc'] + ['misc']\n",
    "    #-------------------------------------\n",
    "    #info = mne.create_info(ch_names, 250, ch_types=ch_types)\n",
    "    #eeg_mne = mne.io.array.RawArray(signal.T, info)\n",
    "    signal_events = np.concatenate([signal, t_stim, t_type],1)\n",
    "    info_events = mne.create_info(ch_names_events,250, ch_types_events)\n",
    "    eeg = mne.io.RawArray(signal_events.T, info_events)\n",
    "    #-------------------------------------\n",
    "    #fig=eeg.plot_psd()\n",
    "    eeg.filter(1,20)\n",
    "    #fig=eeg.plot_psd()\n",
    "    event_times = mne.find_events(eeg, stim_channel='t_type') \n",
    "    print(f'Eventos coincidentes encontrados (sin tmin & tmax), {event_times[:,0].size}', file=file_temp) # También se puede obtener con {len(event_times)}\n",
    "    print(f'IDs de los eventos, {np.max(event_times[:,2])}', file=file_temp)\n",
    "    #eeg.plot(scalings='auto',n_channels=8,events=event_times,block=True)   # scalings=10e-05\n",
    "    #-------------------------------------\n",
    "    if (np.unique(t_flash[:,0]).shape[0] != 4200):                          # evalúa si el \"sample point id\" de t_flash tiene el tamaño correcto.\n",
    "        u,c = np.unique( t_flash[:,0], return_counts=True)                  # u->Los elementos únicos. c->la cantidad de veces que se repiten.\n",
    "        dup = u[c>1]                                                        # dup->Los que estén repetidos mas de una vez.    \n",
    "        \n",
    "        for i in range(dup.shape[0]):\n",
    "            idx = np.where( t_flash[:,0] == dup[i] )[0][0]\n",
    "            t_flash[idx,0]  -= 1\n",
    "            t_flash[idx,1]  = 1\n",
    "            t_type[t_flash[idx,0]] = t_flash[idx,3]\n",
    "            t_stim[t_flash[idx,0]] = t_flash[idx,2]\n",
    "\n",
    "    np.unique(t_flash[:,0]).shape\n",
    "    assert  np.unique(t_flash[:,0]).shape[0] == 4200, 'Problem with experiment structure.  There aren''t enough events.'\n",
    "    #-------------------------------------    \n",
    "    def getstims(eeg_mne, eeg_events):                                      # Get the stimulations.  These are the FLASHINGS of rows and columns.\n",
    "        tmin = 0 \n",
    "        tmax = 0.8\n",
    "        reject = None\n",
    "        event_times = mne.find_events(eeg_events, stim_channel='t_stim',shortest_event=0, verbose=True, min_duration=0.000001, consecutive=True)\n",
    "        print(f'Eventos coincidentes encontrados (con tmin:0 & tmax:0.8), {event_times[:,0].size}', file=file_temp) # También funciona con {len(event_times)}\n",
    "        print(f'IDs de los eventos, {np.max(event_times[:,2])}', file=file_temp)\n",
    "                        \n",
    "        event_id = {'Row1':1,'Row2':2,'Row3':3,'Row4':4,'Row5':5,'Row6':6,'Col1':7,'Col2':8,'Col3':9,'Col4':10,'Col5':11,'Col6':12}\n",
    "\n",
    "        epochs = mne.Epochs(eeg_mne, event_times, event_id, tmin, tmax, proj=False,\n",
    "                        baseline=None, reject=reject, preload=True,\n",
    "                        verbose=True, reject_by_annotation=None)\n",
    "        \n",
    "        stims = event_times[:,-1]\n",
    "        return [epochs,stims]\n",
    "    stimepochs, stims = getstims(eeg, eeg)\n",
    "    #-------------------------------------    \n",
    "    def getlabels(eeg_mne, eeg_events, event_id):\n",
    "        print(f'\\n-----------------------------------')\n",
    "        # Get the hit/no hits labels.  These are the FLASHINGS of rows and columns but selected if they are the ones that will trigger the P300 response or not.\n",
    "        #event_id = { 'first':1, 'second':2 }\n",
    "        #baseline = (0.0, 0.2)\n",
    "        #reject = {'eeg': 70 * pow(10,6)}\n",
    "        tmin = 0\n",
    "        tmax = 0.8\n",
    "        reject = None\n",
    "        event_times = mne.find_events(eeg_events, stim_channel='t_type', shortest_event=0, verbose=True, min_duration=0.000001, consecutive=True)\n",
    "        epochs = mne.Epochs(eeg_mne, event_times, event_id, tmin, tmax, proj=False,\n",
    "                        baseline=None, reject=reject, preload=True,\n",
    "                        verbose=True, reject_by_annotation=None)\n",
    "        labels = epochs.events[:, -1]\n",
    "        return [epochs, labels]\n",
    "    #-------------------------------------    \n",
    "    epochs, labels = getlabels(eeg, eeg, {'first':1}) # -> {'first':1}: Nohits.\n",
    "    # Tupla de tres valores: epochs.get_data().shape . Tomo uno por uno\n",
    "    print(f'getlabels Nohits -> epochs, {(epochs.get_data().shape)[0]}', file=file_temp)\n",
    "    # print(f'getlabels Nohits -> Channels, {(epochs.get_data().shape)[1]}', file=file_temp)\n",
    "    print(f'getlabels Nohits -> Time, {(epochs.get_data().shape)[2]}', file=file_temp)\n",
    "    # print(f'getlabels Nohits -> labels (longitud), {len(labels)}', file=file_temp)\n",
    "    # epocked: Objeto de tipo <class 'mne.evoked.EvokedArray'>. \n",
    "    # En epocked.data puedo sacar info pero por ahora no hay nada. \n",
    "    # El profesor lo usa para visualizar: evoked.plot(). De acá se pueden sacar segmentos promedio.\n",
    "    epocked = epochs.average()\n",
    "    #-------------------------------------  \n",
    "    epochs, labels = getlabels(eeg, eeg, {'second':2}) # -> {'second':2}: Hits.\n",
    "    print(f'getlabels Hits -> epochs, {(epochs.get_data().shape)[0]}', file=file_temp)\n",
    "    # print(f'getlabels Hits -> Channels, {(epochs.get_data().shape)[1]}', file=file_temp)\n",
    "    print(f'getlabels Hits -> Time, {(epochs.get_data().shape)[2]}', file=file_temp)\n",
    "    # print(f'getlabels Hits -> labels (longitud), {len(labels)}', file=file_temp)\n",
    "    epocked = epochs.average()\n",
    "    #-------------------------------------\n",
    "    # OJO CON ESTA LÍNEA: JUNTA LOS HITs/NO HITs\n",
    "    epochs, labels = getlabels(eeg, eeg, { 'first':1, 'second':2})\n",
    "    #-------------------------------------\n",
    "    # Downsample the original FS=250 Hz signal to >>> 20 Hz | epochs.resample(20, npad=\"auto\") | stimepochs.resample(20, npad=\"auto\")\n",
    "    repetitions=120\n",
    "    # This is Single Flashing Classification attempt.\n",
    "    from sklearn.preprocessing import  StandardScaler\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn import svm\n",
    "    from sklearn.pipeline import make_pipeline\n",
    "\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, f1_score # support\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "    # import a linear classifier from mne.decoding\n",
    "    from mne.decoding import LinearModel\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    clf = LogisticRegression(solver='lbfgs')\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # create a linear model with LogisticRegression\n",
    "    model = LinearModel(clf)\n",
    "\n",
    "    # Get the epoched data (get only the data columns)\n",
    "    eeg_data = epochs.get_data().reshape(len(labels), -1)\n",
    "    eeg_data = eeg_data[:,0:epochs.get_data().shape[2]*1]\n",
    "    #eeg_data[labels==2] = erptemplate1[:201,0]\n",
    "    #eeg_data[labels==1] = erptemplate1[:201,0]\n",
    "\n",
    "    #eeg_data[labels==2] = np.zeros((eeg_data.shape[1],))\n",
    "    #eeg_data[labels==1] = np.ones((eeg_data.shape[1],))\n",
    "\n",
    "    #labels = np.random.permutation(labels)\n",
    "\n",
    "    # fit the classifier on MEG data\n",
    "    X = scaler.fit_transform(eeg_data)\n",
    "\n",
    "    model.fit(X[0:2800], labels[0:2800])\n",
    "\n",
    "    preds = model.predict(X[2800:])\n",
    "\n",
    "    # Classification report\n",
    "    target_names = ['nohit', 'hit']\n",
    "\n",
    "    report = classification_report(labels[2800:], preds, target_names=target_names)\n",
    "    print(report)\n",
    "    print(f'Reporte de clasificación: {report}')\n",
    "    cm = confusion_matrix(labels[2800:], preds)\n",
    "    print(cm)\n",
    "    print(f'Matriz de confusión: {cm}')\n",
    "    cm_normalized = cm.astype(float) / cm.sum(axis=1)[:, np.newaxis]\n",
    "    print(cm_normalized)\n",
    "    print(f'Matriz de confusión normalizada: {cm_normalized}')\n",
    "    acc=(cm[0,0]+cm[1,1])*1.0/(np.sum(cm))\n",
    "    print(acc)\n",
    "    print(f'\"Accuracy\": {acc}')\n",
    "\n",
    "\n",
    "    prec = precision_score(labels[2800:], preds)\n",
    "    print(f'Cálculo de la precisión por separado, sin separar nohit de hit: {prec}')\n",
    "    prec2 = precision_score(labels[2800:], preds, labels=target_names)\n",
    "    print(f'Cálculo de la precisión por separado, separando nohit de hit: {prec2}')\n",
    "\n",
    "    #-------------------------------------\n",
    "    file_temp.close()\n",
    "    #-------------------------------------\n",
    "path2 = './a_results/'\n",
    "columns_list = []\n",
    "\n",
    "for filename in os.listdir(path2):\n",
    "    if filename.startswith('Resultados Paciente') and filename.endswith('.csv'):\n",
    "        df = pd.read_csv(os.path.join(path2, filename), delimiter=',')\n",
    "        columns_list.append(df.iloc[:, 1])\n",
    "\n",
    "resultados = pd.concat(columns_list, axis=1)\n",
    "resultados.columns = [filename[:-4] for filename in os.listdir(path2) if filename.startswith('Resultados Paciente') and filename.endswith('.csv')]\n",
    "df1 = pd.read_csv('./a_results/Resultados Paciente P300S01-lag0.csv', index_col=0)\n",
    "resultados.set_index(df1.index, inplace=True)\n",
    "resultados.to_csv('./a_results/compilado_resultados.csv', index=False)\n",
    "resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(report)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =======================================\n",
    "# Por ahora hasta acá\n",
    "# ======================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #punto_mat=scipy.io.loadmat(path)\n",
    "    #signal = punto_mat['data'][0][0][0] \n",
    "    #t_trials = punto_mat['data'][0][0][3]\n",
    "    #t_flash = punto_mat['data'][0][0][4]\n",
    "    #print(f'ESTOY TESTEANDO EL DRUGSIGNAL{ii+1}')\n",
    "    #signal = DrugSignal(signal, t_flash)\n",
    "    #t_stim = punto_mat['data'][0][0][2]\n",
    "    #t_type = punto_mat['data'][0][0][1]\n",
    "    #-------------------------------------\n",
    "    #ch_names=[ 'Fz'  ,  'Cz',    'P3' ,   'Pz'  ,  'P4'  ,  'PO7'   , 'PO8'   , 'Oz']\n",
    "    #ch_types= ['eeg'] * signal.shape[1]\n",
    "    #ch_names_events = ch_names + ['t_stim']+ ['t_type']\n",
    "    #ch_types_events = ch_types + ['misc'] + ['misc']\n",
    "    #-------------------------------------\n",
    "    #info = mne.create_info(ch_names, 250, ch_types=ch_types)\n",
    "    #eeg_mne = mne.io.array.RawArray(signal.T, info)\n",
    "    #signal_events = np.concatenate([signal, t_stim, t_type],1)\n",
    "    #print('-----------------info_events---------------------')\n",
    "    #info_events = mne.create_info(ch_names_events,250, ch_types_events)\n",
    "    #print(info_events)\n",
    "    #print('-----------------eeg---------------------')\n",
    "    #eeg = mne.io.RawArray(signal_events.T, info_events)\n",
    "    #print(eeg)\n",
    "    #print('-----------------eeg_crudo-----------------')\n",
    "    #eeg_crudo=eeg.get_data()\n",
    "    #df_alex = pd.DataFrame(eeg_crudo)\n",
    "    #<RawArray | 10 x 358372 (1433.\n",
    "    #df_eeg = a_fun.to_df(eeg_crudo)\n",
    "    \n",
    "df_eeg = a_fun.to_df(eeg_crudo)\n",
    "signal_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import mne\n",
    "#mne.set_log_level('WARNING')\n",
    "#import scipy.io\n",
    "#import numpy as np\n",
    "\n",
    "# First load the template.  This is the signal that will be used to DRUG the basal EEG stream.\n",
    "mat = scipy.io.loadmat('./dataset/ERPTemplate.mat')\n",
    "\n",
    "routput = mat['routput']\n",
    "\n",
    "# In this ERPTemplate, there are two different template signals that are good.\n",
    "erptemplate1 = routput[0][7][0][1][0][0][0][7] \n",
    "erptemplate2 = routput[0][7][0][1][0][0][0][0] \n",
    "\n",
    "# The original ERPTemplate dataset has a sampling frequency of 256 so I need to perform a small downsampling to 250 Hz\n",
    "erptemplate1 = np.delete( erptemplate1, range(0,256,43),0)\n",
    "erptemplate2 = np.delete( erptemplate2, range(0,256,43),0)\n",
    "\n",
    "# Use this for testing  (get a ZERO signal)\n",
    "#erptemplate1 = np.zeros((250,8))\n",
    "\n",
    "# Randomize amplitude and jitter.\n",
    "# Find the right locations where this should be inserted in the stream.\n",
    "# Insert the signal mantaining the continiuity of the EEG.\n",
    "def DrugSignal(signal, t_flash):\n",
    "    '''\n",
    "    Randomize amplitude and jitter\n",
    "    Find the right locations where this should be inserted in the stream\n",
    "    Insert the template mantaining the continuity and physiological meaning of the EEG\n",
    "    '''\n",
    "    for i in range(0,4200):\n",
    "        if (t_flash[i,3]==2):\n",
    "            signal[t_flash[i,0]-1:t_flash[i,0]+250-1,:] += (erptemplate1*3)\n",
    "\n",
    "    return signal\n",
    "\n",
    "#========================================================================================\n",
    "#========================================================================================\n",
    "#========================================================================================\n",
    "#========================================================================================\n",
    "#.\n",
    "#.\n",
    "#.\n",
    "#========================================================================================\n",
    "#========================================================================================\n",
    "#========================================================================================\n",
    "#========================================================================================\n",
    "\n",
    "# Now load the basal EEG stream\n",
    "#mat = scipy.io.loadmat('./dataset/p300-subject-25.mat')\n",
    "mat = scipy.io.loadmat('./dataset/itba/P300S01.mat')\n",
    "#mat = scipy.io.loadmat('./dataset/p300-subject-26.mat')\n",
    "#mat = scipy.io.loadmat('/Users/rramele/./GoogleDrive/Data/P300/p300-subject-21.mat')\n",
    "#mat = scipy.io.loadmat('/Users/rramele/./GoogleDrive/Data/P300/p300-subject-06.mat')\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "# coding: latin-1\n",
    "# Data point zero for the eight channels.  Should be in V.\n",
    "signal = mat['data'][0][0][0] \n",
    "print(f'Tamaño de {len(signal)}')\n",
    "#* pow(10,6)\n",
    "\n",
    "# Trials\n",
    "t_trials = mat['data'][0][0][3]\n",
    "\n",
    "# Flash matrix\n",
    "t_flash = mat['data'][0][0][4]\n",
    "\n",
    "signal = DrugSignal(signal, t_flash)\n",
    "\n",
    "t_stim = mat['data'][0][0][2]\n",
    "t_type = mat['data'][0][0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Tamaño de {len(t_trials)}')\n",
    "print(f'Tamaño de {len(t_flash)}')\n",
    "print(f'Tamaño de {len(t_stim)}')\n",
    "print(f'Tamaño de {len(t_type)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(t_type)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Acá puedo revisar esta guia -> [*Guía*](https://github.com/faturita/python-nerv/blob/master/MNE%20BNCI%20Horizon%202020%20Dataset%20008-2014.ipynb)    \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis de DrugSignal.py, parte II  \n",
    "  \n",
    "En *a_analisis_funcion_DrugSignal.ipynb* se llega a drogar la señal con variaciones en la amplitud y fase.  \n",
    "A continuación la preparación de los datos para ser visualizados con la librería de python mne.  \n",
    "  \n",
    "[*Creating data objects from arrays. mne library*](https://mne.tools/stable/creating_from_arrays.html)  \n",
    "  \n",
    "- Una instancia básica de información llamada *info_events*:  \n",
    "Esta contiene el nombre de los canales, el ratio de muestreo y el canal de datos.  \n",
    "Para éste caso no se usará canal de datos, por eso se carga *ch_types_events* con ['misc'].  \n",
    "  \n",
    "- Un objeto \"en crudo\" a partir de un array de numpy llamado *eeg*.  \n",
    "\n",
    "- Un tercer objeto *event_times* que trae la info completa y además agrega el 't_type',\n",
    "profundizado en [*a_analisis_p300subject25_stim&type.ipynb*](a_analisis_p300subject25_stim&type.ipynb).\n",
    "\n",
    "Al finalizar, tendremos un objeto *eeg* que contiene todo:  \n",
    "La señal drogada, *t_stim* y *t_type* y sus respectivos eventos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_names=['Fz','Cz','P3','Pz','P4','PO7','PO8', 'Oz']                   # Los nombres de los canales\n",
    "ch_types= ['eeg'] * signal.shape[1]                                     # Un vector de tamaño 8 con el'eeg' repetido\n",
    "ch_names_events = ch_names + ['t_stim']+ ['t_type']                     # Le agrega  't_stim' y't_type' \n",
    "ch_types_events = ch_types + ['misc'] + ['misc']                        # Le agrega dos 'misc': \n",
    "                                                                        # Channel types, default is 'misc' which is not a data channel. \n",
    "                                                                        # Currently supported fields are ‘ecg’, ‘bio’, ‘stim’, ‘eog’, ‘misc’, \n",
    "                                                                        # ‘seeg’, ‘dbs’, ‘ecog’, ‘mag’, ‘eeg’, ‘ref_meg’, ‘grad’, ‘emg’, ‘hbr’ or ‘hbo’. \n",
    "                                                                        # If str, then all channels are assumed to be of the same type.\n",
    "#info = mne.create_info(ch_names, 250, ch_types=ch_types)\n",
    "#eeg_mne = mne.io.array.RawArray(signal.T, info)\n",
    "\n",
    "signal_events = np.concatenate([signal, t_stim, t_type],1)              # Se agrega a la señal drogada 't_stim' y 't_type'\n",
    "\n",
    "df_signal_events = a_fun.to_df(signal_events)\n",
    "ch_names_events_with_sample = ch_names_events + ['sample']\n",
    "df_signal_events.columns = ch_names_events_with_sample\n",
    "\n",
    "info_events = mne.create_info(ch_names_events,250, ch_types_events)     # Acá crea los eventos. \n",
    "eeg = mne.io.RawArray(signal_events.T, info_events)                     # El objeto en crudo.\n",
    "\n",
    "# Do some basic signal processing (1-20 band pass filter)\n",
    "# *** fig=eeg.plot_psd()\n",
    "eeg.filter(1,20)\n",
    "# *** fig=eeg.plot_psd()\n",
    "event_times = mne.find_events(eeg, stim_channel='t_type')    \n",
    "# *** eeg.plot(scalings='auto',n_channels=8,events=event_times,block=True)   # scalings=10e-05\n",
    "\n",
    "#========================================================================\n",
    "# ChatGPT:\n",
    "# Primero, verifica si la cantidad de valores únicos en la primera columna de la matriz es igual a 4200. \n",
    "# Si no es así, entonces el código busca valores duplicados en la primera columna de la matriz \n",
    "# y disminuye el valor de la fila correspondiente en 1 y establece el valor de la columna 1 en 1. \n",
    "# Luego, establece los valores de las columnas 2 y 3 de la matriz t_type y t_stim según los valores\n",
    "# correspondientes de la columna 3 y 2 de la matriz t_flash.\n",
    "# En la última línea del código, se utiliza la función assert para asegurarse de que la cantidad de valores únicos en la primera columna de la matriz t_flash sea igual a 4200. Si no lo es, se imprimirá un mensaje de error indicando que hay un problema con la estructura del experimento.\n",
    "\n",
    "if (np.unique(t_flash[:,0]).shape[0] != 4200):                          # evalúa si el \"sample point id\" de t_flash tiene el tamaño correcto.\n",
    "    u,c = np.unique( t_flash[:,0], return_counts=True)                  # u->Los elementos únicos. c->la cantidad de veces que se repiten.\n",
    "    dup = u[c>1]                                                        # dup->Los que estén repetidos mas de una vez.    \n",
    "    \n",
    "    for i in range(dup.shape[0]):\n",
    "        idx = np.where( t_flash[:,0] == dup[i] )[0][0]\n",
    "        t_flash[idx,0]  -= 1\n",
    "        t_flash[idx,1]  = 1\n",
    "        t_type[t_flash[idx,0]] = t_flash[idx,3]\n",
    "        t_stim[t_flash[idx,0]] = t_flash[idx,2]\n",
    "\n",
    "np.unique(t_flash[:,0]).shape\n",
    "assert  np.unique(t_flash[:,0]).shape[0] == 4200, 'Problem with experiment structure.  There aren''t enough events.'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gráficas de la señal con el ERP agregado y con *t_stim y t_type*.   \n",
    "La primera gráfica muestra el resultado general.    \n",
    "La segunda gráfica es cada una de las señales en un periodo de tiempo determinado.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(30,8))                                     \n",
    "#axes = plt.gca()\n",
    "#for i in range(0,9):\n",
    "#    plt.plot(df_signal_events['sample'], df_signal_events.iloc[:,i])\n",
    "#axes.set_title('DrugSignal con t_stim y t_type'), axes.title.set_size(30)\n",
    "#axes.set_xlabel(xlabel), axes.set_ylabel(ylabel)\n",
    "#axes.xaxis.label.set_size(20), axes.yaxis.label.set_size(20)\n",
    "#plt.savefig(\"./a_images/DrugSignal_p300subject25[data][0][0][0].jpg\")\n",
    "#plt.grid(), plt.show()\n",
    "                 \n",
    "\n",
    "xlim_general=([7000,15500])\n",
    "ylim_general=([-50,80])\n",
    "eje_x_sample=df_signal_events['sample']\n",
    "fig, (ax0, ax1, ax2, ax3, ax4, ax5, ax6, ax7, ax8, ax9) = plt.subplots(nrows=10, ncols=1,figsize=(30,20))\n",
    "fig.suptitle(\"DrugSignal con t_stim y t_type  \\n Eje x: muestra. Eje y: uV \", fontsize=30)\n",
    "\n",
    "ax0.plot(eje_x_sample, df_signal_events.iloc[:,0], label='0') \n",
    "ax0.set_xlim(xlim_general), ax0.set_ylim(ylim_general), ax0.set_title(df_signal_events.columns[0]), ax0.grid()\n",
    "\n",
    "ax1.plot(eje_x_sample, df_signal_events.iloc[:,1], label='1')\n",
    "ax1.set_xlim(xlim_general), ax1.set_ylim(ylim_general), ax1.set_title(df_signal_events.columns[1]), ax1.grid()\n",
    "\n",
    "ax2.plot(eje_x_sample, df_signal_events.iloc[:,2], label='2')\n",
    "ax2.set_xlim(xlim_general), ax2.set_ylim(ylim_general), ax2.set_title(df_signal_events.columns[2]), ax2.grid()\n",
    "\n",
    "ax3.plot(eje_x_sample, df_signal_events.iloc[:,3], label='3')\n",
    "ax3.set_xlim(xlim_general), ax3.set_ylim(ylim_general), ax3.set_title(df_signal_events.columns[3]), ax3.grid()\n",
    "\n",
    "ax4.plot(eje_x_sample, df_signal_events.iloc[:,4], label='4')\n",
    "ax4.set_xlim(xlim_general), ax4.set_ylim(ylim_general), ax4.set_title(df_signal_events.columns[4]), ax4.grid()\n",
    "\n",
    "ax5.plot(eje_x_sample, df_signal_events.iloc[:,5], label='5')\n",
    "ax5.set_xlim(xlim_general), ax5.set_ylim(ylim_general), ax4.set_title(df_signal_events.columns[5]), ax5.grid()\n",
    "\n",
    "ax6.plot(eje_x_sample, df_signal_events.iloc[:,6], label='6')\n",
    "ax6.set_xlim(xlim_general), ax6.set_ylim(ylim_general), ax6.set_title(df_signal_events.columns[6]), ax6.grid()\n",
    "\n",
    "ax7.plot(eje_x_sample, df_signal_events.iloc[:,7], label='7')\n",
    "ax7.set_xlim(xlim_general), ax7.set_ylim(ylim_general), ax7.set_title(df_signal_events.columns[7]), ax7.grid()\n",
    "\n",
    "ax8.plot(eje_x_sample, df_signal_events.iloc[:,8], label='8')\n",
    "ax8.set_xlim(xlim_general), ax8.set_ylim([-3,14]), ax8.set_title(df_signal_events.columns[8]), ax8.grid()\n",
    "\n",
    "ax9.plot(eje_x_sample, df_signal_events.iloc[:,9], label='9')\n",
    "ax9.set_xlim(xlim_general), ax9.set_ylim([-0.2,2.2]), ax9.set_title(df_signal_events.columns[9]), ax9.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones para obtener las estimulaciones y etiquetas.\n",
    "  \n",
    "#### getstims.  \n",
    "La función usa *info*, que es creada con los nombres de los canales (ch_names), la frecuencia de muestreo (259) y los tipos de canales ch_types.  También toma como entrada *eeg_mne*; es la traspuesta de signal e info.\n",
    "\n",
    "El resultado *event_times* es llamando a la función *find_events* de mne, que encuentra eventos del archivo \"raw\".  \n",
    "\n",
    "[*\"Find events\"*](https://mne.tools/stable/generated/mne.find_events.html)  \n",
    "\n",
    "\n",
    "[*\"Épocas: INVESTIGAR. NO ENTENDI NADA\"*](https://mne.tools/stable/generated/mne.Epochs.html#mne-epochs)\n",
    "\n",
    "#### getlabels.\n",
    "\n",
    "- Una instancia básica de información llamada *info_events*:  \n",
    "Esta contiene el nombre de los canales, el ratio de muestreo y el canal de datos.  \n",
    "Para éste caso no se usará canal de datos, por eso se carga *ch_types_events* con ['misc'].  \n",
    "  \n",
    "- Un objeto \"en crudo\" a partir de un array de numpy llamado *eeg*.  \n",
    "\n",
    "- Un tercer objeto *event_times* que trae la info completa y además agrega el 't_type',\n",
    "profundizado en [*a_analisis_p300subject25_stim&type.ipynb*](a_analisis_p300subject25_stim&type.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#========================================================================\n",
    "# getstims recibe dos argumentos: eeg_mne y eeg_events. eeg_mne debe ser un objeto de tipo MNE Raw o Epochs, \n",
    "# mientras que eeg_events debe ser un objeto que contenga los eventos relacionados con la señal EEG.\n",
    "# La función utiliza la biblioteca MNE de Python para encontrar los eventos en la señal EEG y crear epochs de 0 a 0.8 segundos de duración, \n",
    "# a partir de los eventos encontrados. Luego, la función extrae las marcas de tiempo de los eventos y las devuelve como una lista junto \n",
    "# con los epochs.\n",
    "\n",
    "# En resumen, la función getstims extrae los estímulos de una señal EEG y devuelve una lista de epochs y las marcas de tiempo de los eventos \n",
    "# relacionados con la señal.\n",
    "\n",
    "def getstims(eeg_mne, eeg_events):\n",
    "    '''\n",
    "    Get the stimulations.  These are the FLASHINGS of rows and columns.\n",
    "    '''\n",
    "    tmin = 0\n",
    "    tmax = 0.8\n",
    "    reject = None\n",
    "    event_times = mne.find_events(eeg_events, stim_channel='t_stim',shortest_event=0, verbose=True, min_duration=0.000001, consecutive=True)\n",
    "    event_id = {'Row1':1,'Row2':2,'Row3':3,'Row4':4,'Row5':5,'Row6':6,'Col1':7,'Col2':8,'Col3':9,'Col4':10,'Col5':11,'Col6':12}\n",
    "\n",
    "\n",
    "    epochs = mne.Epochs(eeg_mne, event_times, event_id, tmin, tmax, proj=False,\n",
    "                    baseline=None, reject=reject, preload=True,\n",
    "                    verbose=True, reject_by_annotation=None)\n",
    "\n",
    "\n",
    "    stims = event_times[:,-1]\n",
    "\n",
    "    return [epochs,stims]\n",
    "\n",
    "stimepochs, stims = getstims(eeg, eeg)                                  # Época de estimulación.\n",
    "stimepochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#========================================================================\n",
    "# ChatGPT:\n",
    "# La función getlabels recibe tres argumentos: eeg_mne, eeg_events y event_id. eeg_mne debe ser un objeto de tipo MNE Raw o Epochs, \n",
    "# mientras que eeg_events debe ser un objeto que contenga los eventos relacionados con la señal EEG.\n",
    "# La función utiliza la biblioteca MNE de Python para encontrar los eventos en la señal EEG y crear epochs de 0 a 0.8 segundos de duración, \n",
    "# a partir de los eventos encontrados. Luego, la función extrae las marcas de tiempo de los eventos y las devuelve como una lista junto \n",
    "# con los epochs.\n",
    "\n",
    "# En resumen, la función getstims extrae los estímulos de una señal EEG y devuelve una lista de epochs y las marcas de tiempo de los eventos \n",
    "# relacionados con la señal.\n",
    "\n",
    "\n",
    "def getlabels(eeg_mne, eeg_events, event_id):\n",
    "    '''\n",
    "    Get the hit/no hits labels.  These are the FLASHINGS of rows and columns but selected if they are the ones that will\n",
    "    trigger the P300 response or not.\n",
    "    '''\n",
    "    #event_id = { 'first':1, 'second':2 }\n",
    "    #baseline = (0.0, 0.2)\n",
    "    #reject = {'eeg': 70 * pow(10,6)}\n",
    "    tmin = 0\n",
    "    tmax = 0.8\n",
    "    reject = None\n",
    "    event_times = mne.find_events(eeg_events, stim_channel='t_type', shortest_event=0, verbose=True, min_duration=0.000001, consecutive=True)\n",
    "    epochs = mne.Epochs(eeg_mne, event_times, event_id, tmin, tmax, proj=False,\n",
    "                    baseline=None, reject=reject, preload=True,\n",
    "                    verbose=True, reject_by_annotation=None)\n",
    "    labels = epochs.events[:, -1]\n",
    "    return [epochs, labels]\n",
    "\n",
    "epochs, labels = getlabels(eeg, eeg, {'first':1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "epocked = epochs.average()\n",
    "epocked.plot(window_title='NoHit Averaged Signals')\n",
    "\n",
    "\n",
    "epochs, labels = getlabels(eeg, eeg, {'second':2})\n",
    "\n",
    "epocked = epochs.average()\n",
    "epocked.plot(window_title='Hit Averaged Signals')\n",
    "\n",
    "epochs, labels = getlabels(eeg, eeg, { 'first':1, 'second':2})\n",
    "\n",
    "\n",
    "# Downsample the original FS=250 Hz signal to >>> 20 Hz\n",
    "#epochs.resample(20, npad=\"auto\")\n",
    "#stimepochs.resample(20, npad=\"auto\")\n",
    "repetitions=120"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6b958d2ee4113081da5b7324adcb012071bef3fd829261c203699a43a2ce6bc6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
